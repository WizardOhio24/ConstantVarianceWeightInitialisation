{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPplpSFNDIYu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F2Rpv0WpDFnW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmenzies\\Miniconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision.datasets\n",
    "\n",
    "from sklearn import linear_model\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FfhogyZaEdKO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U-XDIEiGLrMe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def XavierInitialisation(network):\n",
    "    \"\"\" Perform Xavier Initialisation on a Neural Network \"\"\"\n",
    "    for p in network.modules():\n",
    "        if p.__class__.__name__ == \"Linear\" or p.__class__.__name__ == \"Conv2d\":\n",
    "            with torch.no_grad():\n",
    "                nn.init.xavier_uniform_(p.weight)\n",
    "                p.bias.zero_()\n",
    "\n",
    "def XavierInitialisationNormal(network):\n",
    "    \"\"\" Perform Xavier Initialisation on a Neural Network \"\"\"\n",
    "    for p in network.modules():\n",
    "        if p.__class__.__name__ == \"Linear\" or p.__class__.__name__ == \"Conv2d\":\n",
    "            with torch.no_grad():\n",
    "                nn.init.xavier_normal_(p.weight)\n",
    "                p.bias.zero_()\n",
    "\n",
    "# All the coefficients for maintaining a constant variance propagation\n",
    "normalising_coeff_dict = {\n",
    "    #nn.Sigmoid: 1.92,\n",
    "    nn.Tanh: 1.58,\n",
    "    nn.ReLU: 1.41,\n",
    "    #nn.Softsign: 2.317374,\n",
    "    #nn.ELU: 1.242043\n",
    "}\n",
    "\n",
    "normalising_coeff_arr = normalising_coeff_dict.values()\n",
    "\n",
    "activation_functions = [\n",
    "    nn.ReLU,\n",
    "    nn.Tanh,\n",
    "    nn.Sigmoid,\n",
    "    nn.SELU,\n",
    "    nn.GELU\n",
    "]\n",
    "\n",
    "def ConstVarInitialisation(network, normalising_coefficient=None, normal=False):\n",
    "    \"\"\" Perform Constant Variance Initialisation on a Neural Network \"\"\"\n",
    "    net_modules = list(network.modules())\n",
    "    for i, p in enumerate(network.modules()):\n",
    "        if p.__class__.__name__ == \"Linear\" and net_modules[\n",
    "            i - 1].__class__ in activation_functions or p.__class__.__name__ == \"Conv2d\" and normalising_coefficient is not None:\n",
    "            with torch.no_grad():\n",
    "                receptive_field_size = 1\n",
    "                if p.weight.dim() > 2:\n",
    "                    for s in p.weight.shape[2:]:\n",
    "                        receptive_field_size *= s\n",
    "                if normalising_coefficient is None:\n",
    "                    # Get what the coefficient should be by looking at the applied activation before this layer\n",
    "                    normalising_coefficient = normalising_coeff_dict[net_modules[i - 1].__class__]\n",
    "                if normal:\n",
    "                  p.weight.normal_(0, normalising_coefficient / np.sqrt(p.weight.shape[1]*receptive_field_size))\n",
    "                else:\n",
    "                  p.weight.uniform_(\n",
    "                      -normalising_coefficient / np.sqrt(p.weight.shape[1]*receptive_field_size), \n",
    "                      normalising_coefficient / np.sqrt(p.weight.shape[1]*receptive_field_size)\n",
    "                      )\n",
    "                # Fill bias with zeros\n",
    "                p.bias.zero_()\n",
    "\n",
    "\n",
    "def test_backpropagation(neural_network, data_loader, loss_fn):\n",
    "    \"\"\" Get the backpropagation distributions from running a number of inputs to the network \"\"\"\n",
    "    handles = []\n",
    "    layerID = 0\n",
    "    backpropagation_values = {}\n",
    "    # Set hooks\n",
    "    def set_backward_hooks(name):\n",
    "        def hook(model, input, output):\n",
    "            backpropagation_values[name].append(output[0].detach())\n",
    "        return hook\n",
    "\n",
    "    # Initialise hook storage holders\n",
    "    for m in neural_network.modules():\n",
    "        if m.__class__.__name__ == \"Linear\" or m.__class__.__name__ == \"Conv2d\":\n",
    "            handles.append(m.register_full_backward_hook(set_backward_hooks(layerID)))\n",
    "            backpropagation_values[layerID] = []\n",
    "            layerID+=1\n",
    "\n",
    "    # Run a few inputs through the network and measure backpropagation with hooks\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        pred = neural_network(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        if batch > 20:\n",
    "            break\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    return backpropagation_values\n",
    "\n",
    "def test_backpropagation_scaled(neural_network, data_loader, loss_fn):\n",
    "    \"\"\" Get the backpropagation distributions from running a number of inputs to the network \"\"\"\n",
    "    handles = []\n",
    "    layerID = 0\n",
    "    backpropagation_values = {}\n",
    "    # Set hooks\n",
    "    def set_backward_hooks(name):\n",
    "        def hook(model, input, output):\n",
    "            backpropagation_values[name].append(output[0].detach())\n",
    "        return hook\n",
    "\n",
    "    # Initialise hook storage holders\n",
    "    for m in neural_network.modules():\n",
    "        if m.__class__.__name__ == \"Linear\":\n",
    "            handles.append(m.register_full_backward_hook(set_backward_hooks(layerID)))\n",
    "            backpropagation_values[layerID] = []\n",
    "            layerID+=1\n",
    "\n",
    "    # Run a few inputs through the network and measure backpropagation with hooks\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Compute how to scale the backpropagation to ensure it is constant variance the backpropagation which ensures\n",
    "        pred = neural_network(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Get the variance of the gradient of the 2nd last module (layer before output layer), then mimic that over all layers\n",
    "        print(neural_network.modules())\n",
    "        propagated_var = np.var(neural_network.modules()[-2].grad)\n",
    "        backpropagation_values.append(neural_network.modules()[0])\n",
    "        for i, m in enumerate(neural_network.modules()[1:]):\n",
    "            m.grad *= propagated_var/m.grad #/neural_network.modules()[i-1]\n",
    "            backpropagation_values.append(m.grad)\n",
    "\n",
    "        if batch > 20:\n",
    "            break\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    return backpropagation_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oLLEMMM_6mVo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_forward_propagation(neural_network, data, loss_fn, device, transform=torchvision.transforms.Compose([]), name=\"Tanh\"):\n",
    "    # Handles for the layer hooks\n",
    "    handles = []\n",
    "\n",
    "    # Register a forward hook to get the value of each layer\n",
    "    layer_intermediate_values = {}\n",
    "    def set_forward_hooks(name):\n",
    "        def hook(model, input, output):\n",
    "            layer_intermediate_values[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    layerID = 0\n",
    "    for m in neural_network.modules():\n",
    "        if m.__class__.__name__ == \"Linear\" or m.__class__.__name__ == \"Conv2d\" or m.__class__.__name__ ==  name:\n",
    "            handles.append(m.register_forward_hook(set_forward_hooks(layerID)))\n",
    "            layerID += 1\n",
    "\n",
    "    # Need to run network to get intermediate values\n",
    "    _out = neural_network(transform(torch.stack([data[d][0] for d in range(64)]).to(device)))\n",
    "\n",
    "    # Remove all handles\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    del handles\n",
    "    del _out\n",
    "\n",
    "    # Return the intermediate values in the layers\n",
    "    return layer_intermediate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jSYMN5Mv6_tm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEuhxyophjXM",
    "outputId": "81e3253a-610a-4264-f80b-1f215e7a10d1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# The CIFAR10 dataset\n",
    "cifar10_dataset_augmented = datasets.CIFAR10(\n",
    "    root=\"./datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=Compose([\n",
    "        ToTensor(),\n",
    "        lambda x: torchvision.transforms.Normalize(torch.mean(x), torch.std(x), inplace=False)(x)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# The dataset applied through the training transform\n",
    "train_transform = transform=Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "# The dataset applied through the validation transform\n",
    "validation_transform = transform=Compose([])\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "cifar10_dataset_train, cifar10_dataset_validation = torch.utils.data.random_split(cifar10_dataset_augmented, [45000, 5000])\n",
    "\n",
    "# Create dataloaders\n",
    "cifar10_dataset_loader_train = DataLoader(cifar10_dataset_train, batch_size=64, shuffle=True)\n",
    "cifar10_dataset_loader_validation = DataLoader(cifar10_dataset_validation, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_o857BLWhhex",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The CIFAR10 dataset, where a half std is applied to the input (instead of standard normalisation)\n",
    "\n",
    "# The dataset applied through the training transform\n",
    "train_transform_half_std = transform=Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    lambda x: torchvision.transforms.Normalize(torch.mean(x), torch.std(x), inplace=True)(x) * 0.5\n",
    "])\n",
    "\n",
    "# The dataset applied through the validation transform\n",
    "validation_transform_half_std = transform=Compose([\n",
    "    lambda x: torchvision.transforms.Normalize(torch.mean(x), torch.std(x), inplace=True)(x) * 0.5\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ViZ9aTnL6rRs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 20 Layer Network almost identical to VGG-19, except padding was added to allow the input to reach the output\n",
    "class NNCIFAR10(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural network for use with the CIFAR10 dataset.\n",
    "    A 19 Layer Network almost identical to VGG-19, except padding was added to allow the input to reach the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function):\n",
    "        super(NNCIFAR10, self).__init__()\n",
    "        self.module_stack = nn.Sequential(\n",
    "            # Input = layer 0\n",
    "            nn.Conv2d(3, 64, (3, 3), padding=2), # 1\n",
    "            activation_function(),\n",
    "            nn.Conv2d(64, 64, (3, 3), padding=2), # 2\n",
    "            activation_function(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(64, 128, (3, 3), padding=2), # 3\n",
    "            activation_function(),\n",
    "            nn.Conv2d(128, 128, (3, 3), padding=2), # 4\n",
    "            activation_function(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(128, 256, (3, 3), padding=2), # 5\n",
    "            activation_function(),\n",
    "            nn.Conv2d(256, 256, (3, 3), padding=2), # 6\n",
    "            activation_function(),\n",
    "            nn.Conv2d(256, 256, (3, 3), padding=2), # 7\n",
    "            activation_function(),\n",
    "            nn.Conv2d(256, 256, (3, 3), padding=2), # 8\n",
    "            activation_function(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(256, 512, (3, 3), padding=2), # 9\n",
    "            activation_function(),\n",
    "            nn.Conv2d(512, 512, (3, 3), padding=2), # 10\n",
    "            activation_function(),\n",
    "            nn.Conv2d(512, 512, (3, 3), padding=2), # 11\n",
    "            activation_function(),\n",
    "            nn.Conv2d(512, 512, (3, 3), padding=2), # 12\n",
    "            activation_function(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Conv2d(512, 512, (3, 3), padding=0), # 13\n",
    "            activation_function(),\n",
    "            nn.Conv2d(512, 512, (3, 3), padding=0), # 14\n",
    "            activation_function(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(2048, 2048), # 15\n",
    "            activation_function(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(2048, 2048), # 16\n",
    "            activation_function(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 1024), # 17\n",
    "            activation_function(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 10), # 18\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WyUCNQ4SEdPF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7bo0dsNqiOd5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(network, \n",
    "                  optimiser, \n",
    "                  dataset_loader_train, \n",
    "                  dataset_loader_validation, \n",
    "                  loss_arr, acc_validation, \n",
    "                  epochs=40,\n",
    "                  train_transform=train_transform, \n",
    "                  validation_transform=validation_transform):\n",
    "  \"\"\" Train a network on the CIFAR10 dataset \"\"\"\n",
    "  lr = linear_model.LinearRegression()\n",
    "  num_epochs = 0\n",
    "  batch_count = 0\n",
    "\n",
    "  for i in range(epochs):\n",
    "      num_epochs += 1\n",
    "      print()\n",
    "      print(\"=================================\")\n",
    "      print(\"Epoch: \" + str(num_epochs))\n",
    "      print(\"=================================\")\n",
    "      for batch, (X, y) in enumerate(dataset_loader_train):\n",
    "          loss =  loss_fn(network(train_transform(X).to(device)), y.to(device)) #, inplace=True\n",
    "          optimiser.zero_grad()\n",
    "          loss.backward()\n",
    "          optimiser.step()\n",
    "\n",
    "          # Print\n",
    "          if batch % print_every == 0:\n",
    "              cur_acc_val = get_accuracy(network, dataset_loader_validation, device, validation_transform)\n",
    "              acc_validation.append([cur_acc_val])\n",
    "              if batch_count > 25:\n",
    "                acc_grad = lr.fit(np.arange(25).reshape(-1, 1), acc_validation[-25:]).coef_[0]\n",
    "                print(\"\\rloss: \" + str(loss.item()) + \" | acc: \" + str(cur_acc_val) + \" | acc_grad: \" + str(acc_grad) + \"                        \", end=\"\")\n",
    "                if acc_grad < 0.00001:\n",
    "                  # Validation accuracy has stopped improving, decrease learning rate\n",
    "                  for g in optimiser.param_groups:\n",
    "                    g['lr'] = g['lr'] * 0.1\n",
    "                    print()\n",
    "                    print(\"Decreased Learning Rate by a factor of 10\")\n",
    "                    batch_count = 10\n",
    "              else:\n",
    "                batch_count += 1\n",
    "                print(\"\\rloss: \" + str(loss.item()) + \" acc: \" + str(cur_acc_val) + \"                        \", end=\"\")\n",
    "          if batch % store_loss_every == 0:\n",
    "              loss_arr.append(loss.item())\n",
    "\n",
    "def run_test(initialisation, \n",
    "             activation = nn.Tanh, \n",
    "             dataset_loader_train = cifar10_dataset_loader_train, \n",
    "             dataset_loader_validation = cifar10_dataset_loader_validation,\n",
    "             train_transform=train_transform,\n",
    "             validation_transform=validation_transform):\n",
    "  \"\"\" Run an initialisation test on the CIFAR dataset \"\"\"\n",
    "  network = NNCIFAR10(activation).to(device)\n",
    "  initialisation(network)\n",
    "  optimiser = torch.optim.SGD(network.parameters(), lr=0.001, momentum=0.9,\n",
    "                            weight_decay=1e-5)\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  loss_arr = []\n",
    "  acc_validation = []\n",
    "\n",
    "  training_loop(\n",
    "      network, \n",
    "      optimiser, \n",
    "      dataset_loader_train, \n",
    "      dataset_loader_validation,\n",
    "      loss_arr, \n",
    "      acc_validation,\n",
    "      40)\n",
    "  \n",
    "  return loss_arr, acc_validation, network, optimiser\n",
    "\n",
    "def get_accuracy(model, dataloader, device, transform):\n",
    "  \"\"\" Determine the top-1 accuracy of the model using the data in the `dataloader` \"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    acc_total = 0\n",
    "    acc_len = 0\n",
    "    for X,y in dataloader:\n",
    "      acc_total += (torch.argmax(model(transform(X).to(device)), dim=1) == y.to(device)).int().sum()\n",
    "      acc_len += len(y)\n",
    "    final_acc = acc_total/acc_len\n",
    "  model.train()\n",
    "  return final_acc.item()\n",
    "\n",
    "def save_arr(name, arr):\n",
    "  with open(\"/content/drive/MyDrive/Datasets/CIFARDataset/\" + name, \"w+\") as f:\n",
    "    f.write(\",\".join([str(i) for i in arr]))\n",
    "    \n",
    "def save_net(name, network):\n",
    "  torch.save(network, \"/content/drive/MyDrive/Datasets/CIFARDataset/\" + name)\n",
    "\n",
    "def save_arr_local(name, arr, itera=0):\n",
    "  with open(\"./HeCIFAR_test/\" + str(itera) + \"/\" + name, \"w+\") as f:\n",
    "    f.write(\",\".join([str(i) for i in arr]))\n",
    "    \n",
    "def save_net_local(name, network, itera=0):\n",
    "  torch.save(network, \"./HeCIFAR_test/\" + str(itera) + \"/\" + name)\n",
    "\n",
    "def load_arr_local(name, arr):\n",
    "  with open(\"./\" + name, \"w+\") as f:\n",
    "    return [float(i) for i in f.read().split(\",\")]\n",
    "    \n",
    "def load_net_local(name, network):\n",
    "  return torch.load(network, \"./\" + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dDQoUVoPNx6F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MKcaahjMNx8U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNAOTOt3EdSy",
    "outputId": "bf767512-564b-4b4d-e56d-2d96f5a5334e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "num_epochs = 0\n",
    "batch_count = 0\n",
    "print_every = 100\n",
    "store_loss_every = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LbwkBi2PJQk_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SptV_h7cEdVd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cW6INGwNS4aQ",
    "outputId": "ca557e41-4336-4907-ed36-a1edb6d0a250",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5888805389404297 acc: 0.36159998178482056                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.4336286783218384 acc: 0.4777999818325043                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.3920990228652954 acc: 0.515999972820282                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.257878303527832 | acc: 0.5719999670982361 | acc_grad: [0.00880231]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.1796770095825195 | acc: 0.6173999905586243 | acc_grad: [0.00633092]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.065219521522522 | acc: 0.6340000033378601 | acc_grad: [0.00549708]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8841147422790527 | acc: 0.6678000092506409 | acc_grad: [0.00456246]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7855876088142395 | acc: 0.6681999564170837 | acc_grad: [0.00311831]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7103956937789917 | acc: 0.7143999934196472 | acc_grad: [0.00289492]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7164866328239441 | acc: 0.7181999683380127 | acc_grad: [0.00249785]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.6365061402320862 | acc: 0.7347999811172485 | acc_grad: [0.00237754]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5538396835327148 | acc: 0.7504000067710876 | acc_grad: [0.00188185]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.5984175205230713 | acc: 0.746399998664856 | acc_grad: [0.00122185]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5072264671325684 | acc: 0.7680000066757202 | acc_grad: [0.00098985]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.684235692024231 | acc: 0.769599974155426 | acc_grad: [0.00083077]                           \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.16683924198150635 | acc: 0.7793999910354614 | acc_grad: [0.00105631]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.40728211402893066 | acc: 0.7857999801635742 | acc_grad: [0.00077815]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.2855188548564911 | acc: 0.7745999693870544 | acc_grad: [0.00074862]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.33094489574432373 | acc: 0.7924000024795532 | acc_grad: [0.00039446]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.29008957743644714 | acc: 0.7802000045776367 | acc_grad: [8.69239752e-05]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.1768002212047577 | acc: 0.7833999991416931 | acc_grad: [0.00032631]                             \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.29108232259750366 | acc: 0.7997999787330627 | acc_grad: [0.00012354]                            \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.09257704019546509 | acc: 0.7961999773979187 | acc_grad: [0.00030862]                            \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.3980751931667328 | acc: 0.8001999855041504 | acc_grad: [0.00023877]                             \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.3359125554561615 | acc: 0.777999997138977 | acc_grad: [-3.2923405e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.13699813187122345 acc: 0.8121999502182007                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.09570787847042084 acc: 0.8159999847412109                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.14587336778640747 | acc: 0.8145999908447266 | acc_grad: [0.00113077]                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.09545043855905533 | acc: 0.8172000050544739 | acc_grad: [0.00015277]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.0824936106801033 | acc: 0.8145999908447266 | acc_grad: [1.38420325e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.2502025067806244 acc: 0.8154000043869019                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.005524030886590481 acc: 0.8155999779701233                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.011653568595647812 | acc: 0.8159999847412109 | acc_grad: [-2.84618598e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01723361201584339 acc: 0.8167999982833862                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.022169820964336395 acc: 0.81659996509552                          \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.1541246771812439 | acc: 0.81659996509552 | acc_grad: [4.73845005e-05]                           \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.04727888107299805 | acc: 0.81659996509552 | acc_grad: [4.92270176e-06]                           \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.1382851004600525 acc: 0.81659996509552                          \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.027829540893435478 acc: 0.81659996509552                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.012849070131778717 | acc: 0.81659996509552 | acc_grad: [7.6857897e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03282744437456131 acc: 0.81659996509552                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.023770928382873535 acc: 0.81659996509552                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.04870520904660225 | acc: 0.81659996509552 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.008342700079083443 acc: 0.81659996509552                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.04875347018241882 acc: 0.81659996509552                         \n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7671786546707153 acc: 0.37459999322891235                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.6093052625656128 acc: 0.4407999813556671                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.254699468612671 acc: 0.5220000147819519                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.398329496383667 | acc: 0.5875999927520752 | acc_grad: [0.00855769]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.1904573440551758 | acc: 0.6308000087738037 | acc_grad: [0.00692031]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.9547128081321716 | acc: 0.6416000127792358 | acc_grad: [0.00549323]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9260977506637573 | acc: 0.676800012588501 | acc_grad: [0.00433877]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7632225751876831 | acc: 0.7143999934196472 | acc_grad: [0.00394462]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7155811190605164 | acc: 0.7005999684333801 | acc_grad: [0.00269154]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5662181377410889 | acc: 0.7328000068664551 | acc_grad: [0.00244108]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.736961841583252 | acc: 0.7361999750137329 | acc_grad: [0.00220385]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.4780575633049011 | acc: 0.7561999559402466 | acc_grad: [0.00203185]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.517266571521759 | acc: 0.7680000066757202 | acc_grad: [0.00145892]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.6776129603385925 | acc: 0.7689999938011169 | acc_grad: [0.00079662]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.5628142356872559 | acc: 0.7856000065803528 | acc_grad: [0.001076]                           \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.45724397897720337 | acc: 0.7673999667167664 | acc_grad: [0.00063031]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.5036841034889221 | acc: 0.7784000039100647 | acc_grad: [0.00059815]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.4054897427558899 | acc: 0.7784000039100647 | acc_grad: [0.00094446]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.37442994117736816 | acc: 0.7841999530792236 | acc_grad: [0.00063046]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.49424102902412415 | acc: 0.7889999747276306 | acc_grad: [0.00027215]                            \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.4150521755218506 | acc: 0.7937999963760376 | acc_grad: [0.00027538]                             \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.25764837861061096 | acc: 0.8025999665260315 | acc_grad: [0.00056815]                           \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.20003363490104675 | acc: 0.798799991607666 | acc_grad: [0.00036185]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.27755334973335266 | acc: 0.795799970626831 | acc_grad: [-1.23079006e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.08136847615242004 acc: 0.818399965763092                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.07244069874286652 acc: 0.8193999528884888                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.07163561135530472 | acc: 0.8179999589920044 | acc_grad: [0.00043815]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.017950458452105522 | acc: 0.8179999589920044 | acc_grad: [8.61456761e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.11596555262804031 acc: 0.81659996509552                           \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.05892719700932503 acc: 0.8179999589920044                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.05222653970122337 | acc: 0.8181999921798706 | acc_grad: [-2.07695136e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08642522990703583 acc: 0.8181999921798706                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.06252395361661911 acc: 0.8181999921798706                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.06879081577062607 | acc: 0.8187999725341797 | acc_grad: [4.70781326e-05]                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.034189336001873016 | acc: 0.8187999725341797 | acc_grad: [3.87695202e-05]                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.05709346756339073 | acc: 0.8187999725341797 | acc_grad: [1.09220927e-05]                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.05860346555709839 | acc: 0.8185999989509583 | acc_grad: [3.07610402e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016745630651712418 acc: 0.8185999989509583                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.03262598440051079 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.06994390487670898 | acc: 0.8187999725341797 | acc_grad: [-4.00034281e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0230221189558506 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.022650718688964844 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.05628107488155365 | acc: 0.8187999725341797 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10976698994636536 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0392184741795063 acc: 0.8187999725341797                          \n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.81138014793396 acc: 0.319599986076355                            \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.4464340209960938 acc: 0.45879998803138733                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.2319550514221191 acc: 0.5365999937057495                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.352205514907837 | acc: 0.5884000062942505 | acc_grad: [0.01002877]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0167217254638672 | acc: 0.576200008392334 | acc_grad: [0.00689815]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.1397569179534912 | acc: 0.6491999626159668 | acc_grad: [0.00457646]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8935904502868652 | acc: 0.6825999617576599 | acc_grad: [0.00488523]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8038458228111267 | acc: 0.6895999908447266 | acc_grad: [0.00430554]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7716500163078308 | acc: 0.7145999670028687 | acc_grad: [0.00296692]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6539301872253418 | acc: 0.7351999878883362 | acc_grad: [0.00251723]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.7203736305236816 | acc: 0.7601999640464783 | acc_grad: [0.00205061]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.6662543416023254 | acc: 0.75 | acc_grad: [0.00180031]                                      \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.5172120928764343 | acc: 0.7619999647140503 | acc_grad: [0.00132631]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4104567766189575 | acc: 0.7657999992370605 | acc_grad: [0.00074138]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.6885917782783508 | acc: 0.7805999517440796 | acc_grad: [0.00070138]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.4241546094417572 | acc: 0.7775999903678894 | acc_grad: [0.00099462]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.491190105676651 | acc: 0.7955999970436096 | acc_grad: [0.00081954]                          \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.3675917983055115 | acc: 0.7993999719619751 | acc_grad: [0.00045769]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.6117169260978699 | acc: 0.7949999570846558 | acc_grad: [0.00069846]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.5918124914169312 | acc: 0.7877999544143677 | acc_grad: [0.00025923]                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.2277667224407196 | acc: 0.7803999781608582 | acc_grad: [-8.70771591e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.30980509519577026 acc: 0.8229999542236328                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.09220251441001892 acc: 0.8273999691009521                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.1724255234003067 | acc: 0.8295999765396118 | acc_grad: [0.00196015]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.1721472591161728 | acc: 0.8258000016212463 | acc_grad: [0.00014954]                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.07108761370182037 | acc: 0.8273999691009521 | acc_grad: [9.99922936e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.11987973749637604 acc: 0.8258000016212463                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.07132182270288467 acc: 0.8281999826431274                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.26663637161254883 | acc: 0.8271999955177307 | acc_grad: [6.78462249e-05]                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.08174244314432144 | acc: 0.8294000029563904 | acc_grad: [8.41541015e-05]                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.1330386996269226 | acc: 0.8273999691009521 | acc_grad: [4.30813202e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07561620324850082 acc: 0.8267999887466431                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.3359120190143585 acc: 0.8269999623298645                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.09108473360538483 | acc: 0.8259999752044678 | acc_grad: [-6.41540839e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0421023815870285 acc: 0.826200008392334                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.11355659365653992 acc: 0.826200008392334                          \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.10494878888130188 | acc: 0.826200008392334 | acc_grad: [-3.52302423e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.06999576836824417 acc: 0.8259999752044678                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.10113022476434708 acc: 0.826200008392334                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.08587674051523209 | acc: 0.826200008392334 | acc_grad: [2.46194693e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06564218550920486 acc: 0.826200008392334                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.09846046566963196 acc: 0.826200008392334                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.06218746677041054 | acc: 0.826200008392334 | acc_grad: [-4.76860083e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.13124071061611176 acc: 0.826200008392334                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.09124057739973068 acc: 0.826200008392334                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.06946947425603867 | acc: 0.826200008392334 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08202848583459854 acc: 0.826200008392334                        \n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.9260034561157227 acc: 0.366599977016449                          \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.553224802017212 acc: 0.47939997911453247                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0663974285125732 acc: 0.4931999742984772                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.2482208013534546 | acc: 0.5737999677658081 | acc_grad: [0.00827139]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.3724039793014526 | acc: 0.611799955368042 | acc_grad: [0.006052]                           \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8637731075286865 | acc: 0.6583999991416931 | acc_grad: [0.00564908]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8859468698501587 | acc: 0.6629999876022339 | acc_grad: [0.00457431]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6994050741195679 | acc: 0.7089999914169312 | acc_grad: [0.00394877]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7875226736068726 | acc: 0.703000009059906 | acc_grad: [0.00286446]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.726292610168457 | acc: 0.7251999974250793 | acc_grad: [0.00195692]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5703205466270447 | acc: 0.7411999702453613 | acc_grad: [0.00131462]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.7879529595375061 | acc: 0.7355999946594238 | acc_grad: [0.00165185]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.6007088422775269 | acc: 0.7489999532699585 | acc_grad: [0.00159708]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.46431800723075867 | acc: 0.7551999688148499 | acc_grad: [0.00098708]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.5527564287185669 | acc: 0.7644000053405762 | acc_grad: [0.00137338]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.875687301158905 | acc: 0.7829999923706055 | acc_grad: [0.001256]                           \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.732271671295166 | acc: 0.7874000072479248 | acc_grad: [0.00082446]                          \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.5003970861434937 | acc: 0.7541999816894531 | acc_grad: [0.00049846]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.32308563590049744 | acc: 0.7827999591827393 | acc_grad: [6.46128104e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.346672922372818 acc: 0.8118000030517578                          \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.1897210031747818 acc: 0.8071999549865723                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.33323511481285095 | acc: 0.8095999956130981 | acc_grad: [0.00118077]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.2203357070684433 | acc: 0.8115999698638916 | acc_grad: [4.23068725e-05]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.15904837846755981 | acc: 0.8077999949455261 | acc_grad: [-2.61549766e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.16338317096233368 acc: 0.8121999502182007                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.3048366904258728 acc: 0.812999963760376                          \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.099207803606987 | acc: 0.8125999569892883 | acc_grad: [0.000118]                            \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.2215673327445984 | acc: 0.8139999508857727 | acc_grad: [2.29230752e-05]                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.25076398253440857 | acc: 0.8118000030517578 | acc_grad: [1.53899193e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.12243355810642242 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.11962555348873138 acc: 0.8125999569892883                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.09009921550750732 | acc: 0.8121999502182007 | acc_grad: [-2.49241866e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.24552597105503082 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.07612288743257523 acc: 0.8125999569892883                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.14076648652553558 | acc: 0.8123999834060669 | acc_grad: [-1.69268021e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.2015983909368515 acc: 0.8125999569892883                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.19081133604049683 acc: 0.8123999834060669                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.1728963851928711 | acc: 0.8125999569892883 | acc_grad: [5.23049098e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.20363572239875793 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.12332124263048172 acc: 0.8123999834060669                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.057065367698669434 | acc: 0.8125999569892883 | acc_grad: [-1.3844325e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10079856961965561 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.11335583031177521 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.38247328996658325 | acc: 0.8125999569892883 | acc_grad: [2.61503917e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.11007663607597351 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.1472773402929306 acc: 0.8125999569892883                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.2805521488189697 | acc: 0.8125999569892883 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08770965784788132 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.6866611242294312 acc: 0.3513999879360199                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.6778050661087036 acc: 0.43320000171661377                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.4706288576126099 acc: 0.5085999965667725                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.2968913316726685 | acc: 0.5812000036239624 | acc_grad: [0.00888231]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.292928695678711 | acc: 0.6137999892234802 | acc_grad: [0.00645769]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.0589052438735962 | acc: 0.6237999796867371 | acc_grad: [0.00453569]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.95462566614151 | acc: 0.6782000064849854 | acc_grad: [0.004648]                            \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.833422064781189 | acc: 0.6631999611854553 | acc_grad: [0.00425215]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7493895888328552 | acc: 0.7031999826431274 | acc_grad: [0.00292615]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6142765879631042 | acc: 0.7231999635696411 | acc_grad: [0.00249338]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5982192754745483 | acc: 0.7325999736785889 | acc_grad: [0.00245738]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.691459059715271 | acc: 0.7217999696731567 | acc_grad: [0.001362]                           \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.4895874261856079 | acc: 0.7504000067710876 | acc_grad: [0.00131046]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5410848259925842 | acc: 0.7543999552726746 | acc_grad: [0.00114031]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.5088021159172058 | acc: 0.7687999606132507 | acc_grad: [0.00096246]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.6031064391136169 | acc: 0.7594000101089478 | acc_grad: [0.00066062]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.36302196979522705 | acc: 0.7856000065803528 | acc_grad: [0.00106539]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.6880516409873962 | acc: 0.7851999998092651 | acc_grad: [0.00092046]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.5449193120002747 | acc: 0.7721999883651733 | acc_grad: [0.00031369]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.32634150981903076 | acc: 0.772599995136261 | acc_grad: [-1.26157816e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.23246221244335175 acc: 0.8101999759674072                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.38415980339050293 acc: 0.8087999820709229                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.12813396751880646 | acc: 0.8121999502182007 | acc_grad: [0.00128323]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.24566930532455444 | acc: 0.8121999502182007 | acc_grad: [6.4614782e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.0926918312907219 | acc: 0.8086000084877014 | acc_grad: [8.07696581e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.20951198041439056 | acc: 0.8109999895095825 | acc_grad: [5.84657376e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.10397050529718399 acc: 0.8154000043869019                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.18475548923015594 acc: 0.81659996509552                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.07859694957733154 | acc: 0.8161999583244324 | acc_grad: [0.00018692]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.07672861963510513 | acc: 0.8137999773025513 | acc_grad: [-1.4768793e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07488564401865005 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.03097774274647236 acc: 0.8157999515533447                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.08892293274402618 | acc: 0.8167999982833862 | acc_grad: [6.75381147e-05]                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0942586287856102 | acc: 0.8159999847412109 | acc_grad: [7.29235777e-05]                          \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.12095638364553452 | acc: 0.8155999779701233 | acc_grad: [4.76960952e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0442829430103302 acc: 0.8155999779701233                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.05979400500655174 acc: 0.8155999779701233                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.055655237287282944 | acc: 0.8155999779701233 | acc_grad: [-2.16922393e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.13194513320922852 acc: 0.8157999515533447                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.0933312475681305 acc: 0.8157999515533447                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.14392530918121338 | acc: 0.8159999847412109 | acc_grad: [1.99973583e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03528469055891037 acc: 0.8157999515533447                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.038974083960056305 acc: 0.8159999847412109                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.1013035774230957 | acc: 0.8159999847412109 | acc_grad: [9.23129228e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.17391330003738403 acc: 0.8159999847412109                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    relu_loss_arr, relu_acc_validation, relu_model, relu_optimiser = run_test(lambda x: ConstVarInitialisation(x, 1.4142, normal=True), activation=torch.nn.ReLU)\n",
    "    relu_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"relu_he_loss_arr\", relu_loss_arr, i)\n",
    "    save_arr_local(\"relu_he_acc_validation\", relu_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"relu_he_model\", relu_model, i)\n",
    "    save_net_local(\"relu_he_optimiser\", relu_optimiser, i)\n",
    "\n",
    "    del relu_loss_arr\n",
    "    del relu_acc_validation\n",
    "    del relu_model\n",
    "    del relu_optimiser\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KK2MWOcpkwps",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k0kpad9Kkw6c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAePyEX5iicO",
    "outputId": "2a5346ee-a416-4324-aea7-edee70522b26",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.480309009552002 acc: 0.5027999877929688                          \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.2273972034454346 acc: 0.6229999661445618                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.951956570148468 acc: 0.6687999963760376                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.805854082107544 | acc: 0.7053999900817871 | acc_grad: [0.00743862]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.974303126335144 | acc: 0.7215999960899353 | acc_grad: [0.00476862]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.5012548565864563 | acc: 0.7337999939918518 | acc_grad: [0.00332185]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6713380217552185 | acc: 0.7439999580383301 | acc_grad: [0.00191938]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.4169107973575592 | acc: 0.7541999816894531 | acc_grad: [0.00143615]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5556170344352722 | acc: 0.7673999667167664 | acc_grad: [0.00140692]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.4446567893028259 | acc: 0.7716000080108643 | acc_grad: [0.00130431]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.26715803146362305 | acc: 0.7805999517440796 | acc_grad: [0.00076846]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.2504112720489502 | acc: 0.7603999972343445 | acc_grad: [0.00012]                            \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.16235317289829254 | acc: 0.7663999795913696 | acc_grad: [-0.000238]                             \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.158070370554924 acc: 0.7993999719619751                          \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.0230864230543375 acc: 0.8017999529838562                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.047638315707445145 | acc: 0.8055999875068665 | acc_grad: [0.00133646]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.035733357071876526 | acc: 0.8043999671936035 | acc_grad: [0.00015292]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.007055469788610935 | acc: 0.8082000017166138 | acc_grad: [0.00016831]                            \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.015449585393071175 | acc: 0.8051999807357788 | acc_grad: [0.00015677]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.022522103041410446 | acc: 0.8047999739646912 | acc_grad: [4.15421449e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.008935732766985893 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.008273564279079437 acc: 0.8065999746322632                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.011987254954874516 | acc: 0.8073999881744385 | acc_grad: [6.12307512e-05]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.017308589071035385 | acc: 0.8071999549865723 | acc_grad: [2.87685486e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.00972813367843628 | acc: 0.8065999746322632 | acc_grad: [5.53809679e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.023430027067661285 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.018671933561563492 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.010299044661223888 | acc: 0.8075999617576599 | acc_grad: [1.23077172e-05]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.015403222292661667 | acc: 0.8073999881744385 | acc_grad: [2.15373589e-05]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.010343599133193493 | acc: 0.8073999881744385 | acc_grad: [5.99984939e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00956565048545599 acc: 0.8073999881744385                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.03620980307459831 acc: 0.8073999881744385                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.012353803962469101 | acc: 0.8073999881744385 | acc_grad: [4.46227881e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008350562304258347 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.011902541853487492 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.010566601529717445 | acc: 0.8073999881744385 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016104578971862793 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.023725934326648712 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.025344686582684517 | acc: 0.8073999881744385 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008111702278256416 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.011954745277762413 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.010768710635602474 | acc: 0.8073999881744385 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.017247984185814857 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0065382542088627815 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0307658351957798 | acc: 0.8073999881744385 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.024433592334389687 acc: 0.8073999881744385                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.006448099855333567 acc: 0.8073999881744385                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.3463271856307983 acc: 0.5073999762535095                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.0694448947906494 acc: 0.5916000008583069                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.860603392124176 acc: 0.6577999591827393                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.8901333808898926 | acc: 0.6947999596595764 | acc_grad: [0.00800477]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.7810171246528625 | acc: 0.7134000062942505 | acc_grad: [0.00471892]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.4620782434940338 | acc: 0.7418000102043152 | acc_grad: [0.00337708]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9242835640907288 | acc: 0.743399977684021 | acc_grad: [0.002228]                            \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.43411216139793396 | acc: 0.7608000040054321 | acc_grad: [0.00135938]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.39042988419532776 | acc: 0.765999972820282 | acc_grad: [0.00105862]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.33602285385131836 | acc: 0.7633999586105347 | acc_grad: [0.00082123]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.28781935572624207 | acc: 0.7617999911308289 | acc_grad: [0.00033862]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.24331124126911163 | acc: 0.776199996471405 | acc_grad: [0.00042739]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.1853943020105362 | acc: 0.7745999693870544 | acc_grad: [0.00046338]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.30153489112854004 | acc: 0.7793999910354614 | acc_grad: [0.00041954]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.29450345039367676 | acc: 0.770799994468689 | acc_grad: [0.000152]                           \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.12324999272823334 | acc: 0.7795999646186829 | acc_grad: [0.000182]                           \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.11704586446285248 | acc: 0.7809999585151672 | acc_grad: [6.76893271e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0927802249789238 acc: 0.8014000058174133                          \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.031113671138882637 acc: 0.8019999861717224                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.016276195645332336 | acc: 0.8032000064849854 | acc_grad: [0.00093492]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.006300493609160185 | acc: 0.8039999604225159 | acc_grad: [7.03075757e-05]                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.005085798446089029 | acc: 0.8021999597549438 | acc_grad: [7.84614911e-05]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.00839232001453638 | acc: 0.8047999739646912 | acc_grad: [3.67684548e-05]                          \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.007144156377762556 | acc: 0.8053999543190002 | acc_grad: [3.81535292e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.0024665994569659233 | acc: 0.8043999671936035 | acc_grad: [6.72316551e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.004223393276333809 | acc: 0.8041999936103821 | acc_grad: [3.23176384e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0015592325944453478 acc: 0.8047999739646912                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.002043705666437745 acc: 0.8050000071525574                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.0012881652219220996 | acc: 0.8053999543190002 | acc_grad: [7.69225451e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0036637966986745596 acc: 0.8053999543190002                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0027413663920015097 acc: 0.8051999807357788                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.009952167049050331 | acc: 0.8051999807357788 | acc_grad: [-3.07688346e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002106905449181795 acc: 0.8050000071525574                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0007974610780365765 acc: 0.8051999807357788                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.009786045178771019 | acc: 0.8051999807357788 | acc_grad: [-6.15028235e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0037313695065677166 acc: 0.8051999807357788                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.002766575198620558 acc: 0.8051999807357788                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.003590730018913746 | acc: 0.8050000071525574 | acc_grad: [2.92269083e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004340016283094883 acc: 0.8051999807357788                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.004860670305788517 acc: 0.8051999807357788                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0036339801736176014 | acc: 0.8051999807357788 | acc_grad: [7.69129166e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.0035188253968954086 acc: 0.8051999807357788                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.00270115421153605 acc: 0.8051999807357788                          \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.010446623899042606 | acc: 0.8051999807357788 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05196231231093407 acc: 0.8051999807357788                          \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.005174889694899321 acc: 0.8051999807357788                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.006901769433170557 | acc: 0.8051999807357788 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0023808209225535393 acc: 0.8051999807357788                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.3201706409454346 acc: 0.5108000040054321                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.1363234519958496 acc: 0.5971999764442444                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.8089565634727478 acc: 0.6649999618530273                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9519476890563965 | acc: 0.6937999725341797 | acc_grad: [0.00762723]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.5799450278282166 | acc: 0.7283999919891357 | acc_grad: [0.004948]                          \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.5815255641937256 | acc: 0.7407999634742737 | acc_grad: [0.00309538]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.5494012832641602 | acc: 0.7337999939918518 | acc_grad: [0.00202585]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.46719884872436523 | acc: 0.7522000074386597 | acc_grad: [0.00152477]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.27396178245544434 | acc: 0.743399977684021 | acc_grad: [0.00092]                            \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6655844449996948 | acc: 0.7533999681472778 | acc_grad: [0.00050831]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.38016900420188904 | acc: 0.7163999676704407 | acc_grad: [-1.04618531e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.11823403835296631 acc: 0.7874000072479248                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.11771687120199203 acc: 0.7877999544143677                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.1463467925786972 | acc: 0.7907999753952026 | acc_grad: [0.00128908]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.06390449404716492 | acc: 0.7881999611854553 | acc_grad: [0.00016923]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.07813701033592224 | acc: 0.7874000072479248 | acc_grad: [-4.24612485e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.04685254022479057 acc: 0.7943999767303467                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.03278828784823418 acc: 0.7925999760627747                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.06200297176837921 | acc: 0.79339998960495 | acc_grad: [0.00013231]                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.1277773678302765 | acc: 0.7929999828338623 | acc_grad: [4.92293101e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02743871510028839 acc: 0.7929999828338623                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.03224692493677139 acc: 0.7929999828338623                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.05613617226481438 | acc: 0.793999969959259 | acc_grad: [1.16918179e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.079169362783432 | acc: 0.7937999963760376 | acc_grad: [5.27692758e-05]                          \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.013222225941717625 | acc: 0.7935999631881714 | acc_grad: [9.07700795e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.09256818890571594 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.047356877475976944 acc: 0.7935999631881714                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.05086495354771614 | acc: 0.7937999963760376 | acc_grad: [-4.15430619e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07128680497407913 acc: 0.7937999963760376                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.04116992652416229 acc: 0.793999969959259                          \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.03961789980530739 | acc: 0.7935999631881714 | acc_grad: [4.30785693e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.024334389716386795 acc: 0.7935999631881714                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0602385476231575 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.06888817250728607 | acc: 0.7935999631881714 | acc_grad: [-8.76976893e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.020109044387936592 acc: 0.7935999631881714                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.055419497191905975 acc: 0.7935999631881714                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.045030802488327026 | acc: 0.7935999631881714 | acc_grad: [-1.53871683e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06505563110113144 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.03741879016160965 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.06861214339733124 | acc: 0.7935999631881714 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03529042750597 acc: 0.7935999631881714                            \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.07159797102212906 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.036512188613414764 | acc: 0.7935999631881714 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0396420918405056 acc: 0.7935999631881714                          \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.05938108265399933 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.040005799382925034 | acc: 0.7935999631881714 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04756635054945946 acc: 0.7935999631881714                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0995367169380188 acc: 0.7935999631881714                          \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0541047677397728 | acc: 0.7935999631881714 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03480992093682289 acc: 0.7935999631881714                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.524383783340454 acc: 0.4931999742984772                          \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.1098203659057617 acc: 0.6100000143051147                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.9497566819190979 acc: 0.6729999780654907                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9759753346443176 | acc: 0.699999988079071 | acc_grad: [0.00790338]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.7527841329574585 | acc: 0.7260000109672546 | acc_grad: [0.00512508]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6040235161781311 | acc: 0.7347999811172485 | acc_grad: [0.00329277]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.504933774471283 | acc: 0.7540000081062317 | acc_grad: [0.00175615]                          \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.5649141073226929 | acc: 0.7489999532699585 | acc_grad: [0.001068]                           \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.26755228638648987 | acc: 0.7739999890327454 | acc_grad: [0.00149646]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.45258137583732605 | acc: 0.7599999904632568 | acc_grad: [0.00073323]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.25528693199157715 | acc: 0.75 | acc_grad: [0.00024754]                                      \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.20685003697872162 | acc: 0.7479999661445618 | acc_grad: [-4.61248251e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07886924594640732 acc: 0.7907999753952026                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.032321251928806305 acc: 0.7942000031471252                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.02768363617360592 | acc: 0.7942000031471252 | acc_grad: [0.00116246]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.017292190343141556 | acc: 0.7945999503135681 | acc_grad: [0.00016185]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.0377703532576561 | acc: 0.7955999970436096 | acc_grad: [3.6768684e-05]                           \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.035925257951021194 | acc: 0.7960000038146973 | acc_grad: [-2.61531426e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009829659014940262 acc: 0.7967999577522278                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.026944929733872414 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.013808208517730236 | acc: 0.7965999841690063 | acc_grad: [8.32304588e-05]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.026935646310448647 | acc: 0.7979999780654907 | acc_grad: [1.49227106e-05]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.02759179100394249 | acc: 0.7989999651908875 | acc_grad: [6.39997996e-05]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.015917068347334862 | acc: 0.7983999848365784 | acc_grad: [9.12315112e-05]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.044245559722185135 | acc: 0.7975999712944031 | acc_grad: [-4.1536643e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.015409055165946484 acc: 0.7978000044822693                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.021334625780582428 acc: 0.7979999780654907                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.024765346199274063 | acc: 0.7985999584197998 | acc_grad: [-2.00111132e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02160908654332161 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.015610056929290295 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.02149878442287445 | acc: 0.7985999584197998 | acc_grad: [1.27687821e-05]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.030556803569197655 | acc: 0.7985999584197998 | acc_grad: [7.53815358e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04955623298883438 acc: 0.798799991607666                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.03522831201553345 acc: 0.798799991607666                          \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.024861862882971764 | acc: 0.7985999584197998 | acc_grad: [4.15453544e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012658173218369484 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.024908950552344322 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.06916267424821854 | acc: 0.7985999584197998 | acc_grad: [-3.23144289e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.015449007041752338 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.024226563051342964 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.035346727818250656 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012970779091119766 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.04403768852353096 acc: 0.7985999584197998                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.017968134954571724 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01844424568116665 acc: 0.7985999584197998                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.024434225633740425 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0282744187861681 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01627698540687561 acc: 0.7985999584197998                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.019849754869937897 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.02555273286998272 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009648283012211323 acc: 0.7985999584197998                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.2221612930297852 acc: 0.5209999680519104                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.179579496383667 acc: 0.6236000061035156                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.8335996866226196 acc: 0.6521999835968018                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9794402122497559 | acc: 0.6855999827384949 | acc_grad: [0.00765646]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8586350083351135 | acc: 0.7267999649047852 | acc_grad: [0.00525215]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6070795655250549 | acc: 0.7453999519348145 | acc_grad: [0.00356708]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.722493588924408 | acc: 0.7513999938964844 | acc_grad: [0.00260769]                          \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.3960830867290497 | acc: 0.7698000073432922 | acc_grad: [0.00150185]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.4171295762062073 | acc: 0.774399995803833 | acc_grad: [0.001382]                            \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.46416175365448 | acc: 0.772599995136261 | acc_grad: [0.00080231]                            \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.2721770703792572 | acc: 0.7766000032424927 | acc_grad: [0.000292]                           \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.10572534054517746 | acc: 0.777999997138977 | acc_grad: [0.00027354]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.1889713853597641 | acc: 0.7639999985694885 | acc_grad: [0.00022015]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.03724527359008789 | acc: 0.7785999774932861 | acc_grad: [-0.00021215]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0672687217593193 acc: 0.8057999610900879                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.03636757284402847 acc: 0.8118000030517578                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.02884204313158989 | acc: 0.8097999691963196 | acc_grad: [0.00126692]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.036462221294641495 | acc: 0.8082000017166138 | acc_grad: [0.00010923]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.010194431990385056 | acc: 0.8095999956130981 | acc_grad: [-3.08018464e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009347743354737759 acc: 0.8104000091552734                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.00865466520190239 acc: 0.8109999895095825                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.015173282474279404 | acc: 0.8107999563217163 | acc_grad: [6.95380339e-05]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.007474425248801708 | acc: 0.8121999502182007 | acc_grad: [5.56908663e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.0160544291138649 | acc: 0.8119999766349792 | acc_grad: [5.23072023e-05]                           \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.0068390327505767345 | acc: 0.8118000030517578 | acc_grad: [1.5692986e-05]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.008417847566306591 | acc: 0.8107999563217163 | acc_grad: [1.84623095e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01456644106656313 acc: 0.8115999698638916                          \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.011810041032731533 acc: 0.811199963092804                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.01271478645503521 | acc: 0.8113999962806702 | acc_grad: [-6.00026204e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.014444191008806229 acc: 0.8113999962806702                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.015231098979711533 acc: 0.811199963092804                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.011933598667383194 | acc: 0.8109999895095825 | acc_grad: [-2.92227818e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.019939148798584938 acc: 0.811199963092804                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.014121336862444878 acc: 0.8113999962806702                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.013743078336119652 | acc: 0.8109999895095825 | acc_grad: [-3.07835065e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03244461864233017 acc: 0.811199963092804                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.013441159389913082 acc: 0.811199963092804                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.006571073550730944 | acc: 0.811199963092804 | acc_grad: [-5.23195817e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02074764296412468 acc: 0.811199963092804                          \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.025607304647564888 acc: 0.811199963092804                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.027024708688259125 | acc: 0.811199963092804 | acc_grad: [1.3844325e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009069646708667278 acc: 0.811199963092804                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0072755892761051655 acc: 0.811199963092804                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.03690638393163681 | acc: 0.811199963092804 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.012405440211296082 acc: 0.811199963092804                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.011329472064971924 acc: 0.811199963092804                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.03066227398812771 | acc: 0.811199963092804 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.022195667028427124 acc: 0.811199963092804                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.018082842230796814 acc: 0.811199963092804                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    elu_relu_normal_loss_arr, elu_relu_normal_acc_validation, elu_relu_normal_model, elu_relu_normal_optimiser = run_test(lambda x: ConstVarInitialisation(x, normalising_coefficient=1.4142, normal=True), activation=torch.nn.ELU)\n",
    "    elu_relu_normal_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"elu_relu_normal_loss_arr\", elu_relu_normal_loss_arr, i)\n",
    "    save_arr_local(\"elu_relu_normal_acc_validation\", elu_relu_normal_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"elu_relu_normal_model\", elu_relu_normal_model, i)\n",
    "    save_net_local(\"elu_relu_normal_optimiser\", elu_relu_normal_optimiser, i)\n",
    "\n",
    "    del elu_relu_normal_loss_arr\n",
    "    del elu_relu_normal_acc_validation\n",
    "    del elu_relu_normal_model\n",
    "    del elu_relu_normal_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jsGqwb7Cky35",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xWgi60-8kzwc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "groPgZItky6X",
    "outputId": "37ab5fb9-a699-4333-f07a-bc74f136327b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.2840454578399658 acc: 0.5051999688148499                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.2879178524017334 acc: 0.5839999914169312                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.9903225898742676 acc: 0.6577999591827393                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.814389705657959 | acc: 0.6881999969482422 | acc_grad: [0.00873723]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.660413920879364 | acc: 0.7116000056266785 | acc_grad: [0.00529277]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8099545240402222 | acc: 0.718999981880188 | acc_grad: [0.00330277]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6088966727256775 | acc: 0.7441999912261963 | acc_grad: [0.00265554]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.5110785365104675 | acc: 0.7505999803543091 | acc_grad: [0.00165277]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.46197670698165894 | acc: 0.7599999904632568 | acc_grad: [0.00168031]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.33135178685188293 | acc: 0.7705999612808228 | acc_grad: [0.00106769]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.3519804775714874 | acc: 0.7805999517440796 | acc_grad: [0.00035415]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.2734566330909729 | acc: 0.7791999578475952 | acc_grad: [0.00085154]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.1341664046049118 | acc: 0.7859999537467957 | acc_grad: [0.00055631]                          \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.10495690256357193 | acc: 0.7797999978065491 | acc_grad: [5.20006968e-05]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.15643778443336487 | acc: 0.7777999639511108 | acc_grad: [0.00024292]                            \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.13776470720767975 | acc: 0.7739999890327454 | acc_grad: [5.5380051e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.09361578524112701 acc: 0.8087999820709229                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.026677630841732025 acc: 0.8097999691963196                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.012401513755321503 | acc: 0.8115999698638916 | acc_grad: [0.00139877]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.023968232795596123 | acc: 0.8139999508857727 | acc_grad: [0.00021231]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.006119924131780863 | acc: 0.8137999773025513 | acc_grad: [0.00020338]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.0025716214440762997 | acc: 0.8136000037193298 | acc_grad: [0.00011662]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.007280898746103048 | acc: 0.8145999908447266 | acc_grad: [-6.14523888e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00625455379486084 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.011128480546176434 acc: 0.8154000043869019                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.011567343957722187 | acc: 0.814799964427948 | acc_grad: [6.59995813e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.0028929580003023148 | acc: 0.8154000043869019 | acc_grad: [4.5691912e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.018116112798452377 | acc: 0.8157999515533447 | acc_grad: [2.90763837e-05]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.005459114443510771 | acc: 0.8149999976158142 | acc_grad: [8.00008957e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0023145133163779974 acc: 0.8149999976158142                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.012161283753812313 acc: 0.8149999976158142                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.008623995818197727 | acc: 0.814799964427948 | acc_grad: [-1.32302596e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010033722035586834 acc: 0.814799964427948                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.012445082888007164 acc: 0.814799964427948                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0053682588040828705 | acc: 0.814799964427948 | acc_grad: [-9.69391603e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004951735958456993 acc: 0.814799964427948                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.008281187154352665 acc: 0.814799964427948                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.004550926387310028 | acc: 0.814799964427948 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00246597733348608 acc: 0.814799964427948                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.0028528824914246798 acc: 0.814799964427948                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.006537218578159809 | acc: 0.814799964427948 | acc_grad: [7.69358415e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.00753618311136961 acc: 0.814799964427948                          \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.0047086747363209724 acc: 0.814799964427948                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0026187740731984377 | acc: 0.814799964427948 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.007739038672298193 acc: 0.814799964427948                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0028549358248710632 acc: 0.814799964427948                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0027691356372088194 | acc: 0.814799964427948 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0048215570859611034 acc: 0.814799964427948                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.4143682718276978 acc: 0.4901999831199646                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.0248358249664307 acc: 0.600600004196167                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.9068313241004944 acc: 0.6503999829292297                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9815772771835327 | acc: 0.699999988079071 | acc_grad: [0.008506]                           \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.7520345449447632 | acc: 0.724399983882904 | acc_grad: [0.00566092]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.4130003750324249 | acc: 0.7581999897956848 | acc_grad: [0.00420123]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.5661744475364685 | acc: 0.7576000094413757 | acc_grad: [0.00278723]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8640342950820923 | acc: 0.7621999979019165 | acc_grad: [0.00168615]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.410350501537323 | acc: 0.7752000093460083 | acc_grad: [0.00082554]                          \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.2668463885784149 | acc: 0.7851999998092651 | acc_grad: [0.00076446]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.13776913285255432 | acc: 0.7785999774932861 | acc_grad: [0.00098354]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.2649863064289093 | acc: 0.7749999761581421 | acc_grad: [0.00027415]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.3177971839904785 | acc: 0.786799967288971 | acc_grad: [9.98455286e-05]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.20787638425827026 | acc: 0.7789999842643738 | acc_grad: [7.96922812e-05]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.1325945109128952 | acc: 0.7581999897956848 | acc_grad: [-0.00015338]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.12572944164276123 acc: 0.8075999617576599                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.06858643144369125 acc: 0.8083999752998352                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.03482501208782196 | acc: 0.8095999956130981 | acc_grad: [0.00099677]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.006444333121180534 | acc: 0.8118000030517578 | acc_grad: [6.64620216e-05]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.011469865217804909 | acc: 0.8093999624252319 | acc_grad: [3.80003452e-05]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.014109469950199127 | acc: 0.812999963760376 | acc_grad: [3.30766348e-05]                          \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.007117919158190489 | acc: 0.8136000037193298 | acc_grad: [9.52302951e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.004764222539961338 | acc: 0.8131999969482422 | acc_grad: [0.00011169]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.01282996591180563 | acc: 0.8115999698638916 | acc_grad: [2.8000474e-05]                          \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.009988495148718357 | acc: 0.8155999779701233 | acc_grad: [5.41537083e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.009829042479395866 | acc: 0.8137999773025513 | acc_grad: [0.00010031]                             \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.0013896829914301634 | acc: 0.8139999508857727 | acc_grad: [6.41529377e-05]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.0022513442672789097 | acc: 0.8131999969482422 | acc_grad: [-1.06153121e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005059071350842714 acc: 0.8133999705314636                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.006657673045992851 acc: 0.812999963760376                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.0028439180459827185 | acc: 0.8133999705314636 | acc_grad: [-5.27685422e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002469539176672697 acc: 0.8136000037193298                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.002945928368717432 acc: 0.8137999773025513                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.002167775295674801 | acc: 0.8137999773025513 | acc_grad: [3.04614581e-05]                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.007321994286030531 | acc: 0.8137999773025513 | acc_grad: [9.53811866e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.005318368319422007 acc: 0.8137999773025513                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.004203375894576311 acc: 0.8137999773025513                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.003581449156627059 | acc: 0.8137999773025513 | acc_grad: [6.15303333e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004030575975775719 acc: 0.8137999773025513                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.004961345810443163 acc: 0.8137999773025513                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.00965589564293623 | acc: 0.8137999773025513 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002221474889665842 acc: 0.8137999773025513                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.010275332257151604 acc: 0.8137999773025513                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.003602460492402315 | acc: 0.8137999773025513 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006156314164400101 acc: 0.8137999773025513                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0031660532113164663 acc: 0.8137999773025513                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5657097101211548 acc: 0.4675999879837036                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.1071040630340576 acc: 0.6051999926567078                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.1785802841186523 acc: 0.6553999781608582                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.843794584274292 | acc: 0.6911999583244324 | acc_grad: [0.00881831]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.5207383036613464 | acc: 0.7206000089645386 | acc_grad: [0.00632754]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.40297582745552063 | acc: 0.7509999871253967 | acc_grad: [0.00394046]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.507571816444397 | acc: 0.7504000067710876 | acc_grad: [0.00284739]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.3433971107006073 | acc: 0.7579999566078186 | acc_grad: [0.00191092]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6669511198997498 | acc: 0.7573999762535095 | acc_grad: [0.00130754]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.39627572894096375 | acc: 0.7773999571800232 | acc_grad: [0.00062462]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.2937503159046173 | acc: 0.7626000046730042 | acc_grad: [0.00047954]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.21796627342700958 | acc: 0.7784000039100647 | acc_grad: [0.00055492]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.15600517392158508 | acc: 0.7653999924659729 | acc_grad: [-2.26151026e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.04775431007146835 acc: 0.7989999651908875                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.03827303275465965 acc: 0.8014000058174133                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.06416410207748413 | acc: 0.8051999807357788 | acc_grad: [0.00096231]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.021098705008625984 | acc: 0.8025999665260315 | acc_grad: [0.00032323]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.018842194229364395 | acc: 0.8039999604225159 | acc_grad: [0.00011292]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.014651116915047169 | acc: 0.8071999549865723 | acc_grad: [4.55383154e-05]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.01795254275202751 | acc: 0.8065999746322632 | acc_grad: [0.00015338]                             \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.031463347375392914 | acc: 0.8069999814033508 | acc_grad: [9.24621637e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.08702600002288818 | acc: 0.8055999875068665 | acc_grad: [6.30800541e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01369404885917902 acc: 0.8055999875068665                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.01828187331557274 acc: 0.8059999942779541                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.016852978616952896 | acc: 0.8064000010490417 | acc_grad: [-3.69234727e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009847113862633705 acc: 0.8064000010490417                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.016766637563705444 acc: 0.8064000010490417                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.01252820622175932 | acc: 0.8065999746322632 | acc_grad: [2.93849065e-05]                          \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.013553755357861519 | acc: 0.8068000078201294 | acc_grad: [3.53849851e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.018397461622953415 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.009228446520864964 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.018568335101008415 | acc: 0.8068000078201294 | acc_grad: [7.07782232e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009392619132995605 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.018225502222776413 acc: 0.8068000078201294                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.006217702757567167 | acc: 0.8068000078201294 | acc_grad: [1.3844325e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01347524207085371 acc: 0.8068000078201294                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0058873058296740055 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.010154131799936295 | acc: 0.8068000078201294 | acc_grad: [-1.23060667e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016199417412281036 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.008345178328454494 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.008201440796256065 | acc: 0.8068000078201294 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.01941617578268051 acc: 0.8068000078201294                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.007843098603188992 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0036086838226765394 | acc: 0.8068000078201294 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009092152118682861 acc: 0.8068000078201294                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.006294673774391413 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.006038202904164791 | acc: 0.8068000078201294 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004551565740257502 acc: 0.8068000078201294                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.3360235691070557 acc: 0.5115999579429626                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.4679054021835327 acc: 0.602400004863739                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0666029453277588 acc: 0.6523999571800232                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.7843939065933228 | acc: 0.6931999921798706 | acc_grad: [0.00804677]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.9285394549369812 | acc: 0.7295999526977539 | acc_grad: [0.00587123]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8223157525062561 | acc: 0.736799955368042 | acc_grad: [0.00394754]                          \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6167377233505249 | acc: 0.7716000080108643 | acc_grad: [0.00248]                           \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.5247993469238281 | acc: 0.7745999693870544 | acc_grad: [0.00189892]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5674595832824707 | acc: 0.7924000024795532 | acc_grad: [0.00159246]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.3521076440811157 | acc: 0.7821999788284302 | acc_grad: [0.00026754]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.18909795582294464 | acc: 0.7910000085830688 | acc_grad: [0.00030831]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.1510935127735138 | acc: 0.772599995136261 | acc_grad: [0.00043462]                          \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.10880780965089798 | acc: 0.7767999768257141 | acc_grad: [8.80002517e-05]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.11662684381008148 | acc: 0.7963999509811401 | acc_grad: [0.00018815]                           \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.19356073439121246 | acc: 0.786799967288971 | acc_grad: [0.00021831]                             \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.04910808801651001 | acc: 0.7929999828338623 | acc_grad: [-2.96920079e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.057236120104789734 acc: 0.8123999834060669                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.015815369784832 acc: 0.8161999583244324                           \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.0281197652220726 | acc: 0.8181999921798706 | acc_grad: [0.00080554]                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.006499990820884705 | acc: 0.8187999725341797 | acc_grad: [0.00024523]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.005971199832856655 | acc: 0.8193999528884888 | acc_grad: [0.00016446]                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.004050198011100292 | acc: 0.8217999935150146 | acc_grad: [9.36924953e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.007671589497476816 | acc: 0.819599986076355 | acc_grad: [8.0769383e-05]                           \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.007222508080303669 | acc: 0.8219999670982361 | acc_grad: [5.18463667e-05]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.016636408865451813 | acc: 0.823199987411499 | acc_grad: [8.27691188e-05]                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.006796991918236017 | acc: 0.8227999806404114 | acc_grad: [0.00011185]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.004492067266255617 | acc: 0.8219999670982361 | acc_grad: [7.38437359e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0021941273007541895 acc: 0.8217999935150146                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.002684526378288865 acc: 0.8217999935150146                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0037448033690452576 | acc: 0.8217999935150146 | acc_grad: [-1.90766041e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009722543880343437 acc: 0.8217999935150146                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.0030215068254619837 acc: 0.8219999670982361                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0034437940921634436 | acc: 0.8215999603271484 | acc_grad: [1.23005647e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0033576444257050753 acc: 0.8215999603271484                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0035094532649964094 acc: 0.8215999603271484                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.0018330126767978072 | acc: 0.821399986743927 | acc_grad: [-1.12313032e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008882474154233932 acc: 0.8215999603271484                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.004303458612412214 acc: 0.8215999603271484                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0020861055236309767 | acc: 0.8215999603271484 | acc_grad: [-5.38491286e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00420423923060298 acc: 0.8215999603271484                          \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.004756106995046139 acc: 0.8215999603271484                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.0032415310852229595 | acc: 0.8215999603271484 | acc_grad: [4.76860083e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.007503182627260685 acc: 0.8215999603271484                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.005061604082584381 acc: 0.8215999603271484                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.002751454245299101 | acc: 0.8215999603271484 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005141674540936947 acc: 0.8215999603271484                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0022437565494328737 acc: 0.8215999603271484                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5893117189407349 acc: 0.4795999825000763                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.2535896301269531 acc: 0.590399980545044                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.7718331217765808 acc: 0.6481999754905701                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.7896862030029297 | acc: 0.6893999576568604 | acc_grad: [0.00925446]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.46408891677856445 | acc: 0.7135999798774719 | acc_grad: [0.00598477]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6952272057533264 | acc: 0.7378000020980835 | acc_grad: [0.00372077]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.5485475063323975 | acc: 0.7755999565124512 | acc_grad: [0.00247292]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.3796546161174774 | acc: 0.7671999931335449 | acc_grad: [0.00212092]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.32054710388183594 | acc: 0.7845999598503113 | acc_grad: [0.00167569]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.2692279815673828 | acc: 0.7681999802589417 | acc_grad: [0.00080569]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.10044553875923157 | acc: 0.7799999713897705 | acc_grad: [0.00072954]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.23006917536258698 | acc: 0.7770000100135803 | acc_grad: [0.00028]                           \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.12932439148426056 | acc: 0.7838000059127808 | acc_grad: [0.00022677]                            \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.10141772031784058 | acc: 0.7845999598503113 | acc_grad: [0.00047477]                            \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.15318483114242554 | acc: 0.7838000059127808 | acc_grad: [0.00042661]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.033807553350925446 | acc: 0.7910000085830688 | acc_grad: [0.00014246]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.08684977144002914 | acc: 0.7820000052452087 | acc_grad: [-8.69217286e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03190149739384651 acc: 0.8100000023841858                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.0031483187340199947 acc: 0.81659996509552                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.004372658673673868 | acc: 0.8133999705314636 | acc_grad: [0.00105338]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.0033135272096842527 | acc: 0.8141999840736389 | acc_grad: [0.00011692]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.012277034111320972 | acc: 0.812999963760376 | acc_grad: [7.38483209e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004365625325590372 acc: 0.8139999508857727                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.008667629212141037 acc: 0.8136000037193298                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.009617043659090996 | acc: 0.8141999840736389 | acc_grad: [-2.61539221e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0046343873254954815 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.01017683930695057 acc: 0.8143999576568604                          \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.006492041051387787 | acc: 0.8143999576568604 | acc_grad: [2.39993976e-05]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.0050184354186058044 | acc: 0.8143999576568604 | acc_grad: [9.69185279e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.003343241987749934 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.0044678254052996635 acc: 0.8143999576568604                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.00615790206938982 | acc: 0.8143999576568604 | acc_grad: [-7.69908612e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012173964641988277 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.017738711088895798 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.004706279374659061 | acc: 0.8143999576568604 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012099756859242916 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0044927471317350864 acc: 0.8143999576568604                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0034895604476332664 | acc: 0.8143999576568604 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00932821724563837 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.006962919142097235 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.01376864779740572 | acc: 0.8143999576568604 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00716986833140254 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.004124640952795744 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.009527625516057014 | acc: 0.8143999576568604 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012467141263186932 acc: 0.8143999576568604                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.009254680946469307 acc: 0.8143999576568604                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0038420362398028374 | acc: 0.8143999576568604 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.009697189554572105 acc: 0.8143999576568604                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.00707872724160552 acc: 0.8143999576568604                          \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    elu_const_var_loss_arr, elu_const_var_acc_validation, elu_const_var_model, elu_const_var_optimiser = run_test(lambda x: ConstVarInitialisation(x, 1.2453, normal=True), activation=torch.nn.ELU)\n",
    "    elu_const_var_model.to(\"cpu\")\n",
    "    save_arr_local(\"elu_const_var_loss_arr\", elu_const_var_loss_arr, i)\n",
    "    save_arr_local(\"elu_const_var_acc_validation\", elu_const_var_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"elu_const_var_model\", elu_const_var_model, i)\n",
    "    save_net_local(\"elu_const_var_optimiser\", elu_const_var_optimiser, i)\n",
    "\n",
    "    del elu_const_var_loss_arr\n",
    "    del elu_const_var_acc_validation\n",
    "    del elu_const_var_model\n",
    "    del elu_const_var_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_ZVBUomhk7Ai",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7KI3VweTk7Cz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UOywXRYlHtA",
    "outputId": "9239d2f6-1b74-4b8d-df1a-70f79ab50a61",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7239739894866943 acc: 0.34759998321533203                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.5655255317687988 acc: 0.493399977684021                          \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.3200902938842773 acc: 0.5730000138282776                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.1162800788879395 | acc: 0.6011999845504761 | acc_grad: [0.00918308]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0948765277862549 | acc: 0.6363999843597412 | acc_grad: [0.00669031]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8326551914215088 | acc: 0.670199990272522 | acc_grad: [0.00511861]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.7124075889587402 | acc: 0.7077999711036682 | acc_grad: [0.00460646]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7743209600448608 | acc: 0.7239999771118164 | acc_grad: [0.004138]                          \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5661620497703552 | acc: 0.7375999689102173 | acc_grad: [0.00258431]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6846035122871399 | acc: 0.7507999539375305 | acc_grad: [0.00166677]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5263985395431519 | acc: 0.76419997215271 | acc_grad: [0.00175338]                           \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5766967535018921 | acc: 0.7731999754905701 | acc_grad: [0.00176154]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.3179854452610016 | acc: 0.7763999700546265 | acc_grad: [0.00061139]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.6168282628059387 | acc: 0.7811999917030334 | acc_grad: [0.00098231]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3353017270565033 | acc: 0.788599967956543 | acc_grad: [0.00141892]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.301585853099823 | acc: 0.7931999564170837 | acc_grad: [0.00038723]                          \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.3309214115142822 | acc: 0.7821999788284302 | acc_grad: [0.00043585]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.172208771109581 | acc: 0.7960000038146973 | acc_grad: [0.00028938]                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.20028182864189148 | acc: 0.7839999794960022 | acc_grad: [0.00018338]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.18637123703956604 | acc: 0.7833999991416931 | acc_grad: [0.00029461]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.17729084193706512 | acc: 0.7953999638557434 | acc_grad: [6.30695086e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.029393944889307022 acc: 0.814799964427948                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.025831148028373718 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.06999064236879349 | acc: 0.8155999779701233 | acc_grad: [0.00086031]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.015942471101880074 | acc: 0.818399965763092 | acc_grad: [9.78466639e-05]                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.016524840146303177 | acc: 0.8151999711990356 | acc_grad: [1.69258851e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.03339672088623047 acc: 0.8185999989509583                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.04208879917860031 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.006371162366122007 | acc: 0.8172000050544739 | acc_grad: [8.29230364e-05]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.021366022527217865 | acc: 0.8185999989509583 | acc_grad: [-1.53504885e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04794633761048317 acc: 0.8185999989509583                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.022707337513566017 acc: 0.8190000057220459                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.008645094931125641 | acc: 0.8187999725341797 | acc_grad: [4.29232304e-05]                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.01669134758412838 | acc: 0.8191999793052673 | acc_grad: [2.52289497e-05]                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.013669852167367935 | acc: 0.8187999725341797 | acc_grad: [8.15189802e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010048686526715755 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.005002748686820269 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.016730928793549538 | acc: 0.8187999725341797 | acc_grad: [-2.03072566e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005082246381789446 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.0187519658356905 acc: 0.8187999725341797                           \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.07948942482471466 | acc: 0.8190000057220459 | acc_grad: [1.07710178e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0030101595912128687 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.030061455443501472 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.008631742559373379 | acc: 0.8187999725341797 | acc_grad: [-1.53871683e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008931676857173443 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.00754137709736824 acc: 0.8187999725341797                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5135784149169922 acc: 0.3929999768733978                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.3330373764038086 acc: 0.4737999737262726                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.1943399906158447 acc: 0.5515999794006348                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.2481106519699097 | acc: 0.5983999967575073 | acc_grad: [0.00839585]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0124566555023193 | acc: 0.6277999877929688 | acc_grad: [0.00733892]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8624902963638306 | acc: 0.6693999767303467 | acc_grad: [0.00546923]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9656268954277039 | acc: 0.6859999895095825 | acc_grad: [0.00460292]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6203481554985046 | acc: 0.7026000022888184 | acc_grad: [0.00326246]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5639561414718628 | acc: 0.7339999675750732 | acc_grad: [0.00294877]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6117089986801147 | acc: 0.738599956035614 | acc_grad: [0.00274523]                          \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.8675927519798279 | acc: 0.7509999871253967 | acc_grad: [0.00165062]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.39036545157432556 | acc: 0.770799994468689 | acc_grad: [0.00118108]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.6392420530319214 | acc: 0.7730000019073486 | acc_grad: [0.00095554]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4513991177082062 | acc: 0.7703999876976013 | acc_grad: [0.00076046]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.5473159551620483 | acc: 0.7619999647140503 | acc_grad: [0.00032477]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.4193068742752075 | acc: 0.7877999544143677 | acc_grad: [0.00047662]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.3988163471221924 | acc: 0.7789999842643738 | acc_grad: [0.00051138]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.19992969930171967 | acc: 0.7820000052452087 | acc_grad: [0.00047385]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.3760493993759155 | acc: 0.7781999707221985 | acc_grad: [8.30792464e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10655505210161209 acc: 0.8079999685287476                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.13007383048534393 acc: 0.8139999508857727                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.16509270668029785 | acc: 0.8121999502182007 | acc_grad: [0.001592]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.03500810265541077 | acc: 0.8127999901771545 | acc_grad: [0.00011508]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.013969369232654572 | acc: 0.8121999502182007 | acc_grad: [8.55385799e-05]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.027421334758400917 | acc: 0.8123999834060669 | acc_grad: [6.12310263e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.04561324790120125 | acc: 0.8121999502182007 | acc_grad: [-2.76927765e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06376048922538757 acc: 0.8109999895095825                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.034261975437402725 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.016718458384275436 | acc: 0.8125999569892883 | acc_grad: [-5.07735289e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.015003922395408154 acc: 0.8123999834060669                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.08206291496753693 acc: 0.8125999569892883                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.01720237359404564 | acc: 0.8123999834060669 | acc_grad: [-1.53850592e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.007833637297153473 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.03168217092752457 acc: 0.8127999901771545                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.09592246264219284 | acc: 0.8125999569892883 | acc_grad: [6.76929951e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.038502953946590424 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.012860970571637154 acc: 0.8127999901771545                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.03855589032173157 | acc: 0.8125999569892883 | acc_grad: [5.23140797e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.017172925174236298 acc: 0.8125999569892883                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.015597912482917309 acc: 0.8127999901771545                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.02132108435034752 | acc: 0.8127999901771545 | acc_grad: [-2.30752505e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.023150403052568436 acc: 0.8127999901771545                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.020619645714759827 acc: 0.8127999901771545                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.016468990594148636 | acc: 0.8127999901771545 | acc_grad: [2.46194693e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01704743504524231 acc: 0.8127999901771545                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.05470305681228638 acc: 0.8127999901771545                          \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.005008825100958347 | acc: 0.8127999901771545 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.018443450331687927 acc: 0.8127999901771545                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.6958287954330444 acc: 0.3725999891757965                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.4547375440597534 acc: 0.5008000135421753                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.225151777267456 acc: 0.5396000146865845                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9903056025505066 | acc: 0.6146000027656555 | acc_grad: [0.00991092]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.1746329069137573 | acc: 0.6395999789237976 | acc_grad: [0.00582369]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.9701681733131409 | acc: 0.6565999984741211 | acc_grad: [0.00436092]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.7850019931793213 | acc: 0.7067999839782715 | acc_grad: [0.00395938]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.9350376725196838 | acc: 0.7231999635696411 | acc_grad: [0.00340585]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6674555540084839 | acc: 0.7279999852180481 | acc_grad: [0.00252415]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.46607542037963867 | acc: 0.7443999648094177 | acc_grad: [0.00243908]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5645796656608582 | acc: 0.7681999802589417 | acc_grad: [0.00261154]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5458191633224487 | acc: 0.7637999653816223 | acc_grad: [0.00091831]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.45271164178848267 | acc: 0.7687999606132507 | acc_grad: [0.00077062]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4242844879627228 | acc: 0.7655999660491943 | acc_grad: [0.00092185]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.2935997247695923 | acc: 0.7645999789237976 | acc_grad: [0.00072692]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.27460968494415283 | acc: 0.7829999923706055 | acc_grad: [0.00054877]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.285844087600708 | acc: 0.7653999924659729 | acc_grad: [-6.33843587e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.17766845226287842 acc: 0.8079999685287476                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.14274108409881592 acc: 0.8087999820709229                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.06680179387331009 | acc: 0.8089999556541443 | acc_grad: [0.00161123]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.10598019510507584 | acc: 0.8109999895095825 | acc_grad: [6.09233746e-05]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.19890397787094116 | acc: 0.8119999766349792 | acc_grad: [8.15377786e-05]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.032435093075037 | acc: 0.8105999827384949 | acc_grad: [8.23076413e-05]                           \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.0679200291633606 | acc: 0.8075999617576599 | acc_grad: [-3.60002884e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.017184674739837646 acc: 0.8105999827384949                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.057610876858234406 acc: 0.8115999698638916                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.07987207919359207 | acc: 0.8109999895095825 | acc_grad: [3.98460718e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.017515655606985092 | acc: 0.8125999569892883 | acc_grad: [5.53835355e-05]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.007220652420073748 | acc: 0.811199963092804 | acc_grad: [4.58455086e-05]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.22367647290229797 | acc: 0.8115999698638916 | acc_grad: [1.38470301e-05]                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.13811829686164856 | acc: 0.8115999698638916 | acc_grad: [4.46223296e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02883872203528881 acc: 0.8121999502182007                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.06629304587841034 acc: 0.8115999698638916                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.03987111896276474 | acc: 0.8119999766349792 | acc_grad: [8.15359446e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03502291068434715 acc: 0.8119999766349792                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.020396694540977478 acc: 0.8119999766349792                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.047088947147130966 | acc: 0.8119999766349792 | acc_grad: [7.07704287e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.02303297631442547 acc: 0.8119999766349792                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.033144306391477585 acc: 0.8119999766349792                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.040994927287101746 | acc: 0.8119999766349792 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.055224910378456116 acc: 0.8119999766349792                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.03753530979156494 acc: 0.8119999766349792                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.025186963379383087 | acc: 0.8119999766349792 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.12187325954437256 acc: 0.8119999766349792                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.09798146784305573 acc: 0.8119999766349792                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.025547988712787628 | acc: 0.8119999766349792 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03255802392959595 acc: 0.8119999766349792                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7918903827667236 acc: 0.3763999938964844                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.5979403257369995 acc: 0.49379998445510864                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.086145281791687 acc: 0.5527999997138977                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.3063920736312866 | acc: 0.5884000062942505 | acc_grad: [0.00866877]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8652809858322144 | acc: 0.6240000128746033 | acc_grad: [0.00564569]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.0717885494232178 | acc: 0.6642000079154968 | acc_grad: [0.00513277]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6876000165939331 | acc: 0.6865999698638916 | acc_grad: [0.00447738]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7518084645271301 | acc: 0.6855999827384949 | acc_grad: [0.002958]                          \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6127777099609375 | acc: 0.7155999541282654 | acc_grad: [0.00242415]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7426996827125549 | acc: 0.7475999593734741 | acc_grad: [0.00250231]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.6607006192207336 | acc: 0.736799955368042 | acc_grad: [0.00220262]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5698475241661072 | acc: 0.746999979019165 | acc_grad: [0.00142631]                          \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.3324192464351654 | acc: 0.7523999810218811 | acc_grad: [0.00122139]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.44791314005851746 | acc: 0.7795999646186829 | acc_grad: [0.00137908]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.4800046980381012 | acc: 0.7759999632835388 | acc_grad: [0.00034846]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.49368250370025635 | acc: 0.7573999762535095 | acc_grad: [-7.35380558e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.18546263873577118 acc: 0.7911999821662903                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.19269868731498718 acc: 0.7947999835014343                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.11408895254135132 | acc: 0.8001999855041504 | acc_grad: [0.00153092]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.07259543985128403 | acc: 0.7955999970436096 | acc_grad: [0.00025031]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.16483189165592194 | acc: 0.7947999835014343 | acc_grad: [6.15362938e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06508301943540573 acc: 0.7975999712944031                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.07100050896406174 acc: 0.798799991607666                          \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.16554459929466248 | acc: 0.7996000051498413 | acc_grad: [0.00013]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.14409442245960236 | acc: 0.798799991607666 | acc_grad: [4.18458535e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.07797287404537201 | acc: 0.7978000044822693 | acc_grad: [5.07648175e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0960831493139267 acc: 0.7985999584197998                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.12163684517145157 acc: 0.7989999651908875                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.08580227941274643 | acc: 0.7991999983787537 | acc_grad: [4.84620608e-05]                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.19985227286815643 | acc: 0.7991999983787537 | acc_grad: [2.2461827e-05]                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.25360801815986633 | acc: 0.7989999651908875 | acc_grad: [4.30758183e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.12453901022672653 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.06301278620958328 acc: 0.7989999651908875                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.06836789101362228 | acc: 0.7989999651908875 | acc_grad: [-2.030744e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.179783895611763 acc: 0.798799991607666                           \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.03664390742778778 acc: 0.7989999651908875                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.23619917035102844 | acc: 0.798799991607666 | acc_grad: [-5.69251867e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.16859181225299835 acc: 0.798799991607666                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.038728125393390656 acc: 0.798799991607666                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.1548173427581787 | acc: 0.798799991607666 | acc_grad: [-3.07642497e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10756266117095947 acc: 0.798799991607666                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.07303567975759506 acc: 0.798799991607666                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.06449493765830994 | acc: 0.798799991607666 | acc_grad: [-5.07680269e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.16517433524131775 acc: 0.798799991607666                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.07208593189716339 acc: 0.798799991607666                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.02560940757393837 | acc: 0.798799991607666 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.11565832793712616 acc: 0.798799991607666                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.08346837759017944 acc: 0.798799991607666                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.0475528240203857 acc: 0.25200000405311584                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.5155625343322754 acc: 0.4193999767303467                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.5530997514724731 acc: 0.4761999845504761                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0687376260757446 | acc: 0.5238000154495239 | acc_grad: [0.01227338]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.3591728210449219 | acc: 0.5834000110626221 | acc_grad: [0.00819785]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.954216480255127 | acc: 0.6453999876976013 | acc_grad: [0.00642385]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.7483558058738708 | acc: 0.6854000091552734 | acc_grad: [0.00529169]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6439119577407837 | acc: 0.6841999888420105 | acc_grad: [0.00352092]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7947281002998352 | acc: 0.718999981880188 | acc_grad: [0.00311846]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7745568156242371 | acc: 0.7427999973297119 | acc_grad: [0.00280785]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.7403560280799866 | acc: 0.7382000088691711 | acc_grad: [0.00188339]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.4911341667175293 | acc: 0.75 | acc_grad: [0.00108631]                                      \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.6903536915779114 | acc: 0.7657999992370605 | acc_grad: [0.00120708]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4184019863605499 | acc: 0.776199996471405 | acc_grad: [0.00115785]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.26548752188682556 | acc: 0.7821999788284302 | acc_grad: [0.00084877]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.3421814441680908 | acc: 0.7797999978065491 | acc_grad: [0.00078646]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.27541494369506836 | acc: 0.7787999510765076 | acc_grad: [0.00058538]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.22614873945713043 | acc: 0.7795999646186829 | acc_grad: [0.00045292]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.2811132073402405 | acc: 0.786799967288971 | acc_grad: [6.44618273e-05]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.19387109577655792 | acc: 0.769599974155426 | acc_grad: [-6.4307222e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03361697122454643 acc: 0.8079999685287476                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.07040413469076157 acc: 0.8121999502182007                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.027678832411766052 | acc: 0.8104000091552734 | acc_grad: [0.00069062]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.032741840928792953 | acc: 0.8082000017166138 | acc_grad: [1.8462768e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.052752673625946045 acc: 0.8095999956130981                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.024318227544426918 acc: 0.8089999556541443                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.12467623502016068 | acc: 0.8101999759674072 | acc_grad: [-7.15387326e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02878705970942974 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.04328423738479614 acc: 0.8097999691963196                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.032951705157756805 | acc: 0.8093999624252319 | acc_grad: [7.53852037e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.04550237953662872 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.03723638877272606 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.04377153143286705 | acc: 0.8093999624252319 | acc_grad: [-1.26156899e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08797881007194519 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.04384491965174675 acc: 0.8091999888420105                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.021158764138817787 | acc: 0.8093999624252319 | acc_grad: [-3.53799416e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06687310338020325 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.06649181246757507 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.012081306427717209 | acc: 0.8093999624252319 | acc_grad: [4.30712333e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.021617919206619263 acc: 0.8093999624252319                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.033943019807338715 acc: 0.8093999624252319                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.04501313343644142 | acc: 0.8093999624252319 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.025843041017651558 acc: 0.8093999624252319                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.02261502854526043 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.027020271867513657 | acc: 0.8093999624252319 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04300716891884804 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0499722957611084 acc: 0.8093999624252319                          \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.01270707044750452 | acc: 0.8093999624252319 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.038356851786375046 acc: 0.8093999624252319                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    gelu_relu_loss_arr, gelu_relu_acc_validation, gelu_relu_model, gelu_relu_optimiser = run_test(lambda x: ConstVarInitialisation(x, normalising_coefficient=1.4142, normal=True), activation=torch.nn.GELU)\n",
    "    gelu_relu_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"gelu_relu_loss_arr\", gelu_relu_loss_arr, i)\n",
    "    save_arr_local(\"gelu_relu_acc_validation\", gelu_relu_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"gelu_relu_model\", gelu_relu_model, i)\n",
    "    save_net_local(\"gelu_relu_optimiser\", gelu_relu_optimiser, i)\n",
    "\n",
    "    del gelu_relu_loss_arr\n",
    "    del gelu_relu_acc_validation\n",
    "    del gelu_relu_model\n",
    "    del gelu_relu_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YZbr0eOHlH9f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YsMuy5anlH_j",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Je54P2DKk7H9",
    "outputId": "05af742a-796d-44e5-d923-a3fa551a6903",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5837266445159912 acc: 0.43219998478889465                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.3241342306137085 acc: 0.517799973487854                          \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.435797095298767 acc: 0.5465999841690063                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0159192085266113 | acc: 0.592799961566925 | acc_grad: [0.00763308]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0219930410385132 | acc: 0.6484000086784363 | acc_grad: [0.00659169]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.0207669734954834 | acc: 0.6800000071525574 | acc_grad: [0.00476923]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.0933377742767334 | acc: 0.7053999900817871 | acc_grad: [0.00372015]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6805068850517273 | acc: 0.7202000021934509 | acc_grad: [0.00327785]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5108701586723328 | acc: 0.7369999885559082 | acc_grad: [0.00245523]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6718326210975647 | acc: 0.7423999905586243 | acc_grad: [0.00169369]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.6907156109809875 | acc: 0.7260000109672546 | acc_grad: [0.00135739]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.6957301497459412 | acc: 0.7645999789237976 | acc_grad: [0.00110923]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.3084576427936554 | acc: 0.7721999883651733 | acc_grad: [0.00116938]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5039582252502441 | acc: 0.7558000087738037 | acc_grad: [0.00110692]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.28795668482780457 | acc: 0.7815999984741211 | acc_grad: [0.00051738]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.3294375538825989 | acc: 0.7727999687194824 | acc_grad: [0.00078585]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.3543702960014343 | acc: 0.7843999862670898 | acc_grad: [0.00047954]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.3303009271621704 | acc: 0.7805999517440796 | acc_grad: [0.000278]                           \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.6731806397438049 | acc: 0.764799952507019 | acc_grad: [-5.30774318e-05]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.18528032302856445 acc: 0.8011999726295471                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.05974610522389412 acc: 0.8079999685287476                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.06361113488674164 | acc: 0.8071999549865723 | acc_grad: [0.00168508]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.03723074868321419 | acc: 0.8100000023841858 | acc_grad: [0.00031092]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.017533430829644203 | acc: 0.8050000071525574 | acc_grad: [-3.10763487e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.11830443888902664 acc: 0.8071999549865723                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.01659727282822132 acc: 0.8065999746322632                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.03364631161093712 | acc: 0.8068000078201294 | acc_grad: [-1.6307464e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.09399539232254028 acc: 0.8069999814033508                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.012016386725008488 acc: 0.8065999746322632                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.008517211303114891 | acc: 0.8068000078201294 | acc_grad: [3.84647113e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.060776133090257645 acc: 0.8065999746322632                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.04115735739469528 acc: 0.8065999746322632                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0608258917927742 | acc: 0.8065999746322632 | acc_grad: [-1.53816663e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05099422112107277 acc: 0.8068000078201294                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.05190277099609375 acc: 0.8065999746322632                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.026229962706565857 | acc: 0.8068000078201294 | acc_grad: [3.38490193e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06786634773015976 acc: 0.8068000078201294                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.015713078901171684 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.03658212348818779 | acc: 0.8068000078201294 | acc_grad: [1.23147781e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0035518549848347902 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.014985084533691406 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.008604451082646847 | acc: 0.8068000078201294 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.023494519293308258 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.0357525534927845 acc: 0.8068000078201294                          \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.07079914957284927 | acc: 0.8068000078201294 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0181756392121315 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.025322200730443 acc: 0.8068000078201294                           \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.05213966220617294 | acc: 0.8068000078201294 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.8131338357925415 acc: 0.44359999895095825                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.4605005979537964 acc: 0.5022000074386597                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.1325820684432983 acc: 0.5807999968528748                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0613188743591309 | acc: 0.613599956035614 | acc_grad: [0.00775246]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.1132400035858154 | acc: 0.6674000024795532 | acc_grad: [0.00616754]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.832162082195282 | acc: 0.667199969291687 | acc_grad: [0.00476554]                          \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9129849076271057 | acc: 0.7134000062942505 | acc_grad: [0.00422492]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.051596999168396 | acc: 0.6883999705314636 | acc_grad: [0.002924]                           \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6069121360778809 | acc: 0.7269999980926514 | acc_grad: [0.00212092]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.9145417809486389 | acc: 0.7371999621391296 | acc_grad: [0.00140738]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.6605821251869202 | acc: 0.7479999661445618 | acc_grad: [0.00169523]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5971064567565918 | acc: 0.7639999985694885 | acc_grad: [0.00150538]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.4610736072063446 | acc: 0.7649999856948853 | acc_grad: [0.00103492]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5172412395477295 | acc: 0.7635999917984009 | acc_grad: [0.00062338]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.42609870433807373 | acc: 0.7838000059127808 | acc_grad: [0.00046108]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.4494039714336395 | acc: 0.7739999890327454 | acc_grad: [0.00064938]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.3261309266090393 | acc: 0.7889999747276306 | acc_grad: [0.00056908]                             \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.3573552370071411 | acc: 0.7745999693870544 | acc_grad: [0.00024723]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.29657137393951416 | acc: 0.7895999550819397 | acc_grad: [0.000922]                          \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.39966583251953125 | acc: 0.7797999978065491 | acc_grad: [-9.07694376e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.09388939291238785 acc: 0.8051999807357788                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.08380889892578125 acc: 0.8079999685287476                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.046763960272073746 | acc: 0.8086000084877014 | acc_grad: [0.00103354]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.023060761392116547 | acc: 0.8104000091552734 | acc_grad: [0.000104]                              \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.11497162282466888 | acc: 0.8082000017166138 | acc_grad: [7.60003237e-05]                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.019653018563985825 | acc: 0.8101999759674072 | acc_grad: [3.2299757e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.04476388916373253 acc: 0.8101999759674072                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.051903415471315384 acc: 0.8101999759674072                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.016509830951690674 | acc: 0.8091999888420105 | acc_grad: [3.23057175e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02608565241098404 acc: 0.8082000017166138                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.06438189744949341 acc: 0.8083999752998352                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.02669510804116726 | acc: 0.8086000084877014 | acc_grad: [-5.24609364e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.038890641182661057 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.010012040846049786 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.007542778272181749 | acc: 0.8086000084877014 | acc_grad: [4.61596709e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.037816714495420456 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.007467364426702261 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.07218412309885025 | acc: 0.8086000084877014 | acc_grad: [1.53871683e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.028984462842345238 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.01427481509745121 acc: 0.8086000084877014                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.009407570585608482 | acc: 0.8086000084877014 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04659879580140114 acc: 0.8086000084877014                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.047780126333236694 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.008091709576547146 | acc: 0.8086000084877014 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008851509541273117 acc: 0.8086000084877014                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.012671169824898243 acc: 0.8086000084877014                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.4151860475540161 acc: 0.39100000262260437                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.346588134765625 acc: 0.46719998121261597                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.245792031288147 acc: 0.5791999697685242                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0956013202667236 | acc: 0.6139999628067017 | acc_grad: [0.00870585]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.9386817216873169 | acc: 0.6517999768257141 | acc_grad: [0.00590415]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8334140181541443 | acc: 0.6559999585151672 | acc_grad: [0.00419877]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8686588406562805 | acc: 0.7021999955177307 | acc_grad: [0.00444492]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6388687491416931 | acc: 0.7301999926567078 | acc_grad: [0.00317108]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6804500222206116 | acc: 0.7479999661445618 | acc_grad: [0.00274985]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5216121673583984 | acc: 0.7365999817848206 | acc_grad: [0.002352]                          \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.4940144121646881 | acc: 0.7529999613761902 | acc_grad: [0.00146815]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.7237959504127502 | acc: 0.7651999592781067 | acc_grad: [0.00109969]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.45526817440986633 | acc: 0.769599974155426 | acc_grad: [0.000694]                           \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.46826091408729553 | acc: 0.7739999890327454 | acc_grad: [0.00042631]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.5664421319961548 | acc: 0.7845999598503113 | acc_grad: [0.00105923]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.31539037823677063 | acc: 0.7856000065803528 | acc_grad: [0.00065615]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.5549544095993042 | acc: 0.7691999673843384 | acc_grad: [0.00018662]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.40061652660369873 | acc: 0.7487999796867371 | acc_grad: [-0.00022185]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.312115341424942 acc: 0.8079999685287476                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.12731337547302246 acc: 0.8041999936103821                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.04360866919159889 | acc: 0.8046000003814697 | acc_grad: [0.00103554]                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.06546687334775925 | acc: 0.8032000064849854 | acc_grad: [2.15424941e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08721237629652023 acc: 0.8025999665260315                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.08127225190401077 acc: 0.8023999929428101                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.11567037552595139 | acc: 0.8032000064849854 | acc_grad: [-7.72307469e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06514687836170197 acc: 0.8033999800682068                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.07669658958911896 acc: 0.8033999800682068                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.03670347481966019 | acc: 0.8027999997138977 | acc_grad: [1.07714763e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.03879755735397339 acc: 0.8025999665260315                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.06956106424331665 acc: 0.8025999665260315                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.06664970517158508 | acc: 0.8027999997138977 | acc_grad: [-2.27695245e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02485208585858345 acc: 0.8027999997138977                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.08478791266679764 acc: 0.8025999665260315                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.051557507365942 | acc: 0.8027999997138977 | acc_grad: [9.5392649e-06]                            \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.34465962648391724 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.047606971114873886 acc: 0.8029999732971191                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.02347448840737343 | acc: 0.8029999732971191 | acc_grad: [-9.23459346e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04193469509482384 acc: 0.8029999732971191                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.06253509223461151 acc: 0.8029999732971191                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.046726666390895844 | acc: 0.8029999732971191 | acc_grad: [3.69182e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.08378905057907104 acc: 0.8029999732971191                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.02828259952366352 acc: 0.8029999732971191                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.13096405565738678 | acc: 0.8029999732971191 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.047362081706523895 acc: 0.8029999732971191                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0802939385175705 acc: 0.8029999732971191                          \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.038073018193244934 | acc: 0.8029999732971191 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.060331713408231735 acc: 0.8029999732971191                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.035282913595438004 acc: 0.8029999732971191                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.6449236869812012 acc: 0.41659998893737793                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.3219475746154785 acc: 0.5199999809265137                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0650830268859863 acc: 0.5557999610900879                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.1323161125183105 | acc: 0.597000002861023 | acc_grad: [0.00774646]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.116821050643921 | acc: 0.6349999904632568 | acc_grad: [0.00623831]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6313299536705017 | acc: 0.6753999590873718 | acc_grad: [0.00476169]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8114656209945679 | acc: 0.6836000084877014 | acc_grad: [0.00385538]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8288151025772095 | acc: 0.6899999976158142 | acc_grad: [0.00275754]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5111544728279114 | acc: 0.7360000014305115 | acc_grad: [0.00231262]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7498564720153809 | acc: 0.7493999600410461 | acc_grad: [0.00192446]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.39425116777420044 | acc: 0.7501999735832214 | acc_grad: [0.00180677]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.43693915009498596 | acc: 0.7601999640464783 | acc_grad: [0.00131185]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.6521342396736145 | acc: 0.76419997215271 | acc_grad: [0.00095508]                           \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.42801812291145325 | acc: 0.7663999795913696 | acc_grad: [0.001086]                          \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.31904342770576477 | acc: 0.7619999647140503 | acc_grad: [0.00075969]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.20566293597221375 | acc: 0.7831999659538269 | acc_grad: [0.00032031]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.42780232429504395 | acc: 0.7175999879837036 | acc_grad: [-0.00021923]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.1028275117278099 acc: 0.7961999773979187                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.13317088782787323 acc: 0.7978000044822693                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.04921351373195648 | acc: 0.7965999841690063 | acc_grad: [0.00084708]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.18555055558681488 | acc: 0.7999999523162842 | acc_grad: [0.00010231]                           \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.122970350086689 | acc: 0.798799991607666 | acc_grad: [0.00010738]                               \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.025138404220342636 | acc: 0.7965999841690063 | acc_grad: [1.26151855e-05]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.14065499603748322 | acc: 0.7971999645233154 | acc_grad: [-1.72309692e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05054609477519989 acc: 0.7999999523162842                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.07344846427440643 acc: 0.798799991607666                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.07505956292152405 | acc: 0.7991999983787537 | acc_grad: [7.84617204e-05]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.0847223624587059 | acc: 0.7993999719619751 | acc_grad: [-3.84550828e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07479141652584076 acc: 0.7993999719619751                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.1042676642537117 acc: 0.7989999651908875                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.016807567328214645 | acc: 0.7991999983787537 | acc_grad: [-1.21535246e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.025880251079797745 acc: 0.7991999983787537                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.017255324870347977 acc: 0.7991999983787537                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.018947528675198555 | acc: 0.7991999983787537 | acc_grad: [4.30840712e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03342617675662041 acc: 0.7991999983787537                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.13126307725906372 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.03624444082379341 | acc: 0.7991999983787537 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.05711876228451729 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.06253916770219803 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.04571957141160965 | acc: 0.7991999983787537 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04507093504071236 acc: 0.7991999983787537                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.03424585983157158 acc: 0.7991999983787537                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.06083434075117111 | acc: 0.7991999983787537 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06345757097005844 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.04280795529484749 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0661795437335968 | acc: 0.7991999983787537 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.22347579896450043 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.06323053687810898 acc: 0.7991999983787537                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7879254817962646 acc: 0.42419999837875366                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.3732502460479736 acc: 0.51419997215271                          \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.1018810272216797 acc: 0.5719999670982361                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9078207015991211 | acc: 0.6049999594688416 | acc_grad: [0.00773969]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8166018724441528 | acc: 0.6351999640464783 | acc_grad: [0.00620569]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.9091214537620544 | acc: 0.6609999537467957 | acc_grad: [0.00460323]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.7615551352500916 | acc: 0.6881999969482422 | acc_grad: [0.00330415]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.1016287803649902 | acc: 0.7213999629020691 | acc_grad: [0.00346738]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5709802508354187 | acc: 0.7103999853134155 | acc_grad: [0.00280046]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6828212738037109 | acc: 0.7439999580383301 | acc_grad: [0.00165031]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5015528202056885 | acc: 0.7405999898910522 | acc_grad: [0.00172662]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5370878577232361 | acc: 0.7561999559402466 | acc_grad: [0.00164308]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.4003908038139343 | acc: 0.7797999978065491 | acc_grad: [0.00129015]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.3839040696620941 | acc: 0.7653999924659729 | acc_grad: [0.00171246]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3069142997264862 | acc: 0.7675999999046326 | acc_grad: [0.00060692]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.20122647285461426 | acc: 0.7817999720573425 | acc_grad: [4.6153848e-05]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.4034339487552643 | acc: 0.7787999510765076 | acc_grad: [0.00052415]                            \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.17657935619354248 | acc: 0.7731999754905701 | acc_grad: [0.00033077]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.2506062090396881 | acc: 0.7727999687194824 | acc_grad: [7.7692362e-05]                          \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.29034939408302307 | acc: 0.774399995803833 | acc_grad: [-2.38460761e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.14914746582508087 acc: 0.7997999787330627                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.09075293689966202 acc: 0.8014000058174133                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.042799148708581924 | acc: 0.8039999604225159 | acc_grad: [0.00097631]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.04669813811779022 | acc: 0.8017999529838562 | acc_grad: [8.59993696e-05]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.09195825457572937 | acc: 0.7975999712944031 | acc_grad: [3.84546243e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05971204861998558 acc: 0.8001999855041504                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.05781436711549759 acc: 0.8011999726295471                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.06465938687324524 | acc: 0.7996000051498413 | acc_grad: [-5.23035343e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03568553179502487 acc: 0.8007999658584595                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.05237896740436554 acc: 0.8003999590873718                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.018505290150642395 | acc: 0.8009999990463257 | acc_grad: [-3.53842974e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05390932410955429 acc: 0.8009999990463257                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.012340608052909374 acc: 0.8011999726295471                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0771414190530777 | acc: 0.8007999658584595 | acc_grad: [1.907665e-05]                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.01583835855126381 | acc: 0.8007999658584595 | acc_grad: [4.1532975e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08289069682359695 acc: 0.8011999726295471                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.03127787262201309 acc: 0.8009999990463257                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.011545151472091675 | acc: 0.8011999726295471 | acc_grad: [3.07752536e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.017499346286058426 acc: 0.8009999990463257                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.03126957267522812 acc: 0.8011999726295471                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.07088271528482437 | acc: 0.8011999726295471 | acc_grad: [6.15303333e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012976719066500664 acc: 0.8011999726295471                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.04598233476281166 acc: 0.8011999726295471                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.023699065670371056 | acc: 0.8011999726295471 | acc_grad: [1.84591e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03212601691484451 acc: 0.8011999726295471                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.06517302989959717 acc: 0.8011999726295471                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.025667157024145126 | acc: 0.8011999726295471 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.011451896280050278 acc: 0.8011999726295471                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.00966116413474083 acc: 0.8011999726295471                         \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    gelu_const_var_loss_arr, gelu_const_var_acc_validation, gelu_const_var_model, gelu_const_var_optimiser = run_test(lambda x: ConstVarInitialisation(x, 1.5331, normal=True), activation=torch.nn.GELU)\n",
    "    gelu_const_var_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"gelu_const_var_loss_arr\", gelu_const_var_loss_arr, i)\n",
    "    save_arr_local(\"gelu_const_var_acc_validation\", gelu_const_var_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"gelu_const_var_model\", gelu_const_var_model, i)\n",
    "    save_net_local(\"gelu_const_var_optimiser\", gelu_const_var_optimiser, i)\n",
    "\n",
    "    del gelu_const_var_loss_arr\n",
    "    del gelu_const_var_acc_validation\n",
    "    del gelu_const_var_model\n",
    "    del gelu_const_var_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RJZAuVS_iiej",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7hGJS3Y4lr6x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBqhzOGvlyUW",
    "outputId": "c10ed66e-10b4-4829-a1e9-f9b21b6c2d21",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.9155768156051636 acc: 0.2897999882698059                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.5616042613983154 acc: 0.44919997453689575                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.338944911956787 acc: 0.5083999633789062                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.1093645095825195 | acc: 0.5830000042915344 | acc_grad: [0.01157169]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0710248947143555 | acc: 0.6146000027656555 | acc_grad: [0.00808215]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.9262204766273499 | acc: 0.6516000032424927 | acc_grad: [0.00587539]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9672247767448425 | acc: 0.6841999888420105 | acc_grad: [0.00408569]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7887572050094604 | acc: 0.6973999738693237 | acc_grad: [0.00363815]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5908617377281189 | acc: 0.7077999711036682 | acc_grad: [0.00265585]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.49504780769348145 | acc: 0.739799976348877 | acc_grad: [0.00244446]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.6583377122879028 | acc: 0.7265999913215637 | acc_grad: [0.00219692]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.6993422508239746 | acc: 0.7489999532699585 | acc_grad: [0.00149292]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.6054373979568481 | acc: 0.7491999864578247 | acc_grad: [0.00082446]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.7715137600898743 | acc: 0.7599999904632568 | acc_grad: [0.00092262]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.37212517857551575 | acc: 0.764799952507019 | acc_grad: [0.00080477]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.38512226939201355 | acc: 0.7701999545097351 | acc_grad: [0.00028938]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.36967435479164124 | acc: 0.7569999694824219 | acc_grad: [0.00028215]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.3009434938430786 | acc: 0.7753999829292297 | acc_grad: [0.00058108]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.18262556195259094 | acc: 0.7689999938011169 | acc_grad: [0.00010354]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.14768937230110168 | acc: 0.7787999510765076 | acc_grad: [0.00040661]                          \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.18673385679721832 | acc: 0.7723999619483948 | acc_grad: [0.00024369]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.2039165198802948 | acc: 0.7745999693870544 | acc_grad: [0.00018338]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.22104723751544952 | acc: 0.7705999612808228 | acc_grad: [2.0000201e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.03986777365207672 | acc: 0.7838000059127808 | acc_grad: [0.00048062]                           \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.15420734882354736 | acc: 0.7817999720573425 | acc_grad: [0.00021708]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.10601605474948883 | acc: 0.7675999999046326 | acc_grad: [-8.84613165e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07803953438997269 acc: 0.7997999787330627                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.015662124380469322 acc: 0.8007999658584595                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0014960714615881443 | acc: 0.8009999990463257 | acc_grad: [0.00117862]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.00936353113502264 | acc: 0.8025999665260315 | acc_grad: [6.04610718e-05]                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.05102405697107315 | acc: 0.8037999868392944 | acc_grad: [0.00011954]                             \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.001241352059878409 | acc: 0.8047999739646912 | acc_grad: [0.000154]                               \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0028926508966833353 | acc: 0.7996000051498413 | acc_grad: [9.23505196e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004747005645185709 acc: 0.8001999855041504                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.019520001485943794 acc: 0.8019999861717224                          \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.002525713760405779 | acc: 0.8023999929428101 | acc_grad: [-6.84614365e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006728094071149826 acc: 0.8023999929428101                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0005449961754493415 acc: 0.8023999929428101                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.0021808375604450703 | acc: 0.8021999597549438 | acc_grad: [9.38465962e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.003309215186163783 acc: 0.8021999597549438                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.003035543719306588 acc: 0.8021999597549438                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0012225644895806909 | acc: 0.8021999597549438 | acc_grad: [-8.00132751e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00365612655878067 acc: 0.8021999597549438                           \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0064978147856891155 acc: 0.8023999929428101                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.3015782833099365 acc: 0.1005999967455864                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.7807104587554932 acc: 0.31520000100135803                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.660972237586975 acc: 0.4596000015735626                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.211965560913086 | acc: 0.5297999978065491 | acc_grad: [0.01852615]                          \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0946999788284302 | acc: 0.5651999711990356 | acc_grad: [0.01058908]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.8337934017181396 | acc: 0.6069999933242798 | acc_grad: [0.00645554]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9124817252159119 | acc: 0.652999997138977 | acc_grad: [0.00488446]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8082530498504639 | acc: 0.6757999658584595 | acc_grad: [0.00452246]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.808203399181366 | acc: 0.7045999765396118 | acc_grad: [0.00422061]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.8184844851493835 | acc: 0.7295999526977539 | acc_grad: [0.00243615]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.44766828417778015 | acc: 0.7364000082015991 | acc_grad: [0.00222569]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.2892993092536926 | acc: 0.7507999539375305 | acc_grad: [0.00248831]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.7689185738563538 | acc: 0.7447999715805054 | acc_grad: [0.00071308]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5413455367088318 | acc: 0.7595999836921692 | acc_grad: [0.00086554]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.45539385080337524 | acc: 0.7573999762535095 | acc_grad: [0.00140077]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.5920253992080688 | acc: 0.7681999802589417 | acc_grad: [0.00070108]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.46288278698921204 | acc: 0.7567999958992004 | acc_grad: [0.00045723]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.41983306407928467 | acc: 0.772599995136261 | acc_grad: [0.00059339]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.24198418855667114 | acc: 0.7829999923706055 | acc_grad: [0.00023246]                           \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.23310838639736176 | acc: 0.7655999660491943 | acc_grad: [0.00046108]                          \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.14821569621562958 | acc: 0.7770000100135803 | acc_grad: [0.00029462]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.14275053143501282 | acc: 0.7845999598503113 | acc_grad: [0.00021477]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.26152515411376953 | acc: 0.7825999855995178 | acc_grad: [0.00010954]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.2271246612071991 | acc: 0.7821999788284302 | acc_grad: [0.00042]                                \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.07035449892282486 | acc: 0.7879999876022339 | acc_grad: [0.00029892]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.1660158634185791 | acc: 0.7809999585151672 | acc_grad: [-2.60002796e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.04533066973090172 acc: 0.8109999895095825                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.02763892337679863 acc: 0.8133999705314636                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.03279581293463707 | acc: 0.812999963760376 | acc_grad: [0.00082246]                           \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0058009205386042595 | acc: 0.8109999895095825 | acc_grad: [8.39994962e-05]                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0010407887166365981 | acc: 0.8095999956130981 | acc_grad: [4.76924273e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.001325851189903915 acc: 0.8089999556541443                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.01100770290941 acc: 0.8107999563217163                             \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.0024526382330805063 | acc: 0.8104000091552734 | acc_grad: [-4.076866e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00029635828104801476 acc: 0.8104000091552734                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.0038508884608745575 acc: 0.8107999563217163                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.002514729043468833 | acc: 0.8105999827384949 | acc_grad: [1.99993299e-05]                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.015322496183216572 | acc: 0.8107999563217163 | acc_grad: [1.84608881e-05]                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.003903820412233472 | acc: 0.8109999895095825 | acc_grad: [8.77013573e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008710036054253578 acc: 0.8107999563217163                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.00015708338469266891 acc: 0.811199963092804                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0004144794656895101 | acc: 0.8107999563217163 | acc_grad: [5.69196848e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.001292706117965281 acc: 0.8109999895095825                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.012652796693146229 acc: 0.8109999895095825                          \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.2284035682678223 acc: 0.1581999957561493                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.8709805011749268 acc: 0.3651999831199646                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.414984107017517 acc: 0.4479999840259552                          \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.217032790184021 | acc: 0.5231999754905701 | acc_grad: [0.01582154]                          \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.1517635583877563 | acc: 0.5935999751091003 | acc_grad: [0.00906431]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.9479627013206482 | acc: 0.6299999952316284 | acc_grad: [0.00675508]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9066168069839478 | acc: 0.6539999842643738 | acc_grad: [0.00523523]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7969833016395569 | acc: 0.6955999732017517 | acc_grad: [0.00444661]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.8885373473167419 | acc: 0.7185999751091003 | acc_grad: [0.00383108]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 1.0074925422668457 | acc: 0.7383999824523926 | acc_grad: [0.00243815]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.48263800144195557 | acc: 0.7353999614715576 | acc_grad: [0.00215308]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.48900455236434937 | acc: 0.7339999675750732 | acc_grad: [0.00169969]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.6577599048614502 | acc: 0.7579999566078186 | acc_grad: [0.00097215]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.56751948595047 | acc: 0.7651999592781067 | acc_grad: [0.00121723]                           \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3974011242389679 | acc: 0.7671999931335449 | acc_grad: [0.00062477]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.3779701292514801 | acc: 0.7627999782562256 | acc_grad: [0.00057446]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.20622575283050537 | acc: 0.7719999551773071 | acc_grad: [0.00061154]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.4118829071521759 | acc: 0.7797999978065491 | acc_grad: [0.00027831]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.23500478267669678 | acc: 0.7771999835968018 | acc_grad: [0.00062415]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.16107872128486633 | acc: 0.7763999700546265 | acc_grad: [0.00031231]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.20994621515274048 | acc: 0.7667999863624573 | acc_grad: [-6.92340044e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.033297184854745865 acc: 0.8023999929428101                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.07950498163700104 acc: 0.7999999523162842                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.08606239408254623 | acc: 0.8003999590873718 | acc_grad: [0.00095754]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.014127378351986408 | acc: 0.7989999651908875 | acc_grad: [-2.92333273e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.020343385636806488 acc: 0.8001999855041504                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.07349877059459686 acc: 0.8009999990463257                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.010283039882779121 | acc: 0.8027999997138977 | acc_grad: [0.00010092]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.08436424285173416 | acc: 0.8029999732971191 | acc_grad: [0.00013785]                            \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.059918519109487534 | acc: 0.8039999604225159 | acc_grad: [0.00011077]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.045455485582351685 | acc: 0.8051999807357788 | acc_grad: [2.43073702e-05]                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.017141690477728844 | acc: 0.8043999671936035 | acc_grad: [1.53848758e-05]                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.009252727963030338 | acc: 0.8043999671936035 | acc_grad: [9.84627467e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.021369269117712975 acc: 0.8039999604225159                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.05279570445418358 acc: 0.8037999868392944                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.014155167154967785 | acc: 0.8041999936103821 | acc_grad: [-1.07701008e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06028551235795021 acc: 0.8037999868392944                          \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.017179666087031364 acc: 0.8039999604225159                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0018223817460238934 | acc: 0.8037999868392944 | acc_grad: [-6.15991079e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.003937494475394487 acc: 0.8039999604225159                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.006145561579614878 acc: 0.8037999868392944                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.009843945503234863 | acc: 0.8037999868392944 | acc_grad: [-1.07623064e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.003519772319123149 acc: 0.8037999868392944                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.07026761770248413 acc: 0.8037999868392944                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.009566131979227066 | acc: 0.8037999868392944 | acc_grad: [-7.07635513e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10446271300315857 acc: 0.8037999868392944                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0290093831717968 acc: 0.8037999868392944                          \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.1913373470306396 acc: 0.2978000044822693                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.6303714513778687 acc: 0.39959999918937683                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.3793858289718628 acc: 0.5081999897956848                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.3259309530258179 | acc: 0.568399965763092 | acc_grad: [0.010518]                           \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0270657539367676 | acc: 0.6105999946594238 | acc_grad: [0.00782708]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.0177396535873413 | acc: 0.621399998664856 | acc_grad: [0.00574462]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8732288479804993 | acc: 0.6649999618530273 | acc_grad: [0.00440354]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8369343280792236 | acc: 0.7113999724388123 | acc_grad: [0.00434446]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7922807931900024 | acc: 0.7044000029563904 | acc_grad: [0.00271277]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.6397841572761536 | acc: 0.7378000020980835 | acc_grad: [0.00246923]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.4608912169933319 | acc: 0.7547999620437622 | acc_grad: [0.002876]                          \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.5641642808914185 | acc: 0.7680000066757202 | acc_grad: [0.00231477]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.5324471592903137 | acc: 0.7669999599456787 | acc_grad: [0.00136277]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4785320460796356 | acc: 0.7709999680519104 | acc_grad: [0.00076031]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.41371485590934753 | acc: 0.7849999666213989 | acc_grad: [0.00062661]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.5250346660614014 | acc: 0.7766000032424927 | acc_grad: [0.00035939]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.44452324509620667 | acc: 0.7653999924659729 | acc_grad: [0.00019031]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.28621989488601685 | acc: 0.7797999978065491 | acc_grad: [0.00033431]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.1446983963251114 | acc: 0.7771999835968018 | acc_grad: [0.00019846]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.16668124496936798 | acc: 0.7795999646186829 | acc_grad: [-6.95389509e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.18882893025875092 acc: 0.800599992275238                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.049610961228609085 acc: 0.8075999617576599                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.08162220567464828 | acc: 0.7991999983787537 | acc_grad: [0.00082954]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.10156051069498062 | acc: 0.8014000058174133 | acc_grad: [-1.7537979e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07718463242053986 acc: 0.8021999597549438                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.026272138580679893 acc: 0.8035999536514282                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.031492505222558975 | acc: 0.8043999671936035 | acc_grad: [0.00012185]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.04762989282608032 | acc: 0.8047999739646912 | acc_grad: [0.00012292]                             \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.04931080341339111 | acc: 0.8023999929428101 | acc_grad: [2.61563521e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.011426987126469612 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.010959304869174957 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0344117134809494 | acc: 0.8027999997138977 | acc_grad: [-1.3999618e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0563601590692997 acc: 0.8027999997138977                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.07805424928665161 acc: 0.8027999997138977                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.06788262724876404 | acc: 0.8025999665260315 | acc_grad: [-5.83074185e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01653726026415825 acc: 0.8025999665260315                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.05165986344218254 acc: 0.8027999997138977                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.036398179829120636 | acc: 0.8027999997138977 | acc_grad: [-2.76881915e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05132116377353668 acc: 0.8027999997138977                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.020380817353725433 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.04207717254757881 | acc: 0.8027999997138977 | acc_grad: [1.8464602e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.020264577120542526 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.01532103680074215 acc: 0.8027999997138977                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.011642797850072384 | acc: 0.8027999997138977 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.08584654331207275 acc: 0.8027999997138977                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.014307214878499508 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.02950948104262352 | acc: 0.8027999997138977 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.014453300274908543 acc: 0.8027999997138977                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7624655961990356 acc: 0.36959999799728394                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.5705870389938354 acc: 0.4991999864578247                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.2288618087768555 acc: 0.5511999726295471                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0073814392089844 | acc: 0.6083999872207642 | acc_grad: [0.00982692]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.9326386451721191 | acc: 0.6467999815940857 | acc_grad: [0.006262]                          \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.1030080318450928 | acc: 0.6561999917030334 | acc_grad: [0.004428]                          \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.7704015970230103 | acc: 0.69159996509552 | acc_grad: [0.00403]                             \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8610200881958008 | acc: 0.686199963092804 | acc_grad: [0.00304462]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7254502773284912 | acc: 0.7131999731063843 | acc_grad: [0.00223338]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.8526268005371094 | acc: 0.7445999979972839 | acc_grad: [0.00233892]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.6350535154342651 | acc: 0.7342000007629395 | acc_grad: [0.00200523]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.6181653141975403 | acc: 0.739799976348877 | acc_grad: [0.00114308]                          \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.37343040108680725 | acc: 0.774399995803833 | acc_grad: [0.00120323]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.28725653886795044 | acc: 0.7699999809265137 | acc_grad: [0.00092815]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.5457326173782349 | acc: 0.7719999551773071 | acc_grad: [0.00079892]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.534423828125 | acc: 0.7709999680519104 | acc_grad: [0.00099815]                             \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.2579541802406311 | acc: 0.7827999591827393 | acc_grad: [8.67694616e-05]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.2674073874950409 | acc: 0.7753999829292297 | acc_grad: [0.00028723]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.15794117748737335 | acc: 0.7805999517440796 | acc_grad: [0.00033492]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.3074290454387665 | acc: 0.7701999545097351 | acc_grad: [-0.00015385]                            \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.12326116859912872 acc: 0.8001999855041504                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.11704808473587036 acc: 0.8051999807357788                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.025324126705527306 | acc: 0.8055999875068665 | acc_grad: [0.00159092]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.01835581287741661 | acc: 0.8109999895095825 | acc_grad: [0.00030646]                          \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.02757568098604679 | acc: 0.8059999942779541 | acc_grad: [8.8462692e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.03256599232554436 | acc: 0.8100000023841858 | acc_grad: [4.86154281e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.0037394952960312366 | acc: 0.8043999671936035 | acc_grad: [-1.72307858e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01644740253686905 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.01177222654223442 acc: 0.8097999691963196                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.014038603752851486 | acc: 0.8097999691963196 | acc_grad: [9.63068925e-05]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.007604318205267191 | acc: 0.8105999827384949 | acc_grad: [6.13839351e-05]                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.008672697469592094 | acc: 0.8091999888420105 | acc_grad: [-4.92283931e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.023574821650981903 acc: 0.8093999624252319                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0626772791147232 acc: 0.8100000023841858                           \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.01495407335460186 | acc: 0.8093999624252319 | acc_grad: [-1.24616348e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.006810504011809826 acc: 0.8093999624252319                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.0077561987563967705 acc: 0.8091999888420105                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.003863560501486063 | acc: 0.8091999888420105 | acc_grad: [-2.00000635e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0571354404091835 acc: 0.8093999624252319                           \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.006042438093572855 acc: 0.8091999888420105                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.014723305590450764 | acc: 0.8091999888420105 | acc_grad: [-9.99977955e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006683601997792721 acc: 0.8091999888420105                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.008267870172858238 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.013509047217667103 | acc: 0.8091999888420105 | acc_grad: [4.614775e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01798161119222641 acc: 0.8091999888420105                          \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.022771773859858513 acc: 0.8091999888420105                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    silu_relu_loss_arr, silu_relu_acc_validation, silu_relu_model, silu_relu_optimiser = run_test(lambda x: ConstVarInitialisation(x, normalising_coefficient=1.4142, normal=True), activation=torch.nn.SiLU)\n",
    "    silu_relu_model.to(\"cpu\")\n",
    "    save_arr_local(\"silu_relu_loss_arr\", silu_relu_loss_arr, i)\n",
    "    save_arr_local(\"silu_relu_acc_validation\", silu_relu_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"silu_relu_model\", silu_relu_model, i)\n",
    "    save_net_local(\"silu_relu_model\", silu_relu_optimiser, i)\n",
    "\n",
    "    del silu_relu_loss_arr\n",
    "    del silu_relu_acc_validation\n",
    "    del silu_relu_model\n",
    "    del silu_relu_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Pu2GfqJ8lycX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YmPlcEnqlyes",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q60WyhRulr80",
    "outputId": "7077717f-9c14-470b-958d-d36269f88f44",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7536481618881226 acc: 0.4397999942302704                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.1916961669921875 acc: 0.5327999591827393                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.4627113342285156 acc: 0.5740000009536743                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.1275980472564697 | acc: 0.6169999837875366 | acc_grad: [0.00812092]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.1130552291870117 | acc: 0.6491999626159668 | acc_grad: [0.00477815]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.783696174621582 | acc: 0.6909999847412109 | acc_grad: [0.00442846]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.8731233477592468 | acc: 0.7111999988555908 | acc_grad: [0.00391554]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.5483621954917908 | acc: 0.6823999881744385 | acc_grad: [0.00264046]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6165294051170349 | acc: 0.738599956035614 | acc_grad: [0.00192292]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5415131449699402 | acc: 0.7459999918937683 | acc_grad: [0.00165831]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5710441470146179 | acc: 0.7471999526023865 | acc_grad: [0.00203662]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.3980580270290375 | acc: 0.7549999952316284 | acc_grad: [0.00103815]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.2816779911518097 | acc: 0.7561999559402466 | acc_grad: [0.00077569]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4575068950653076 | acc: 0.7590000033378601 | acc_grad: [0.00046338]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3973533511161804 | acc: 0.7730000019073486 | acc_grad: [0.000438]                           \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.3521735966205597 | acc: 0.7716000080108643 | acc_grad: [0.00086092]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.5018790364265442 | acc: 0.7703999876976013 | acc_grad: [0.00034985]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.21529430150985718 | acc: 0.7285999655723572 | acc_grad: [-3.86150984e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04253722354769707 acc: 0.7897999882698059                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.0649227574467659 acc: 0.7870000004768372                          \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.07719257473945618 | acc: 0.7881999611854553 | acc_grad: [0.00066631]                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.08637792617082596 | acc: 0.7913999557495117 | acc_grad: [3.38426003e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.12700912356376648 acc: 0.7893999814987183                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.12610681354999542 acc: 0.7910000085830688                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.06496290117502213 | acc: 0.7901999950408936 | acc_grad: [7.04624561e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.01693951152265072 | acc: 0.7895999550819397 | acc_grad: [9.84595372e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04564886912703514 acc: 0.7897999882698059                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.03144635260105133 acc: 0.7899999618530273                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.040234968066215515 | acc: 0.7895999550819397 | acc_grad: [-1.69249681e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.016651591286063194 acc: 0.7897999882698059                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0524190217256546 acc: 0.7895999550819397                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.021349700167775154 | acc: 0.7895999550819397 | acc_grad: [-1.66163536e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.09897895157337189 acc: 0.7895999550819397                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0658499151468277 acc: 0.7895999550819397                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.07054883241653442 | acc: 0.7895999550819397 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0766979455947876 acc: 0.7895999550819397                          \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.01817457005381584 acc: 0.7895999550819397                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.037356745451688766 | acc: 0.7895999550819397 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07438579201698303 acc: 0.7895999550819397                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.06429964303970337 acc: 0.7895999550819397                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.028725488111376762 | acc: 0.7895999550819397 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.04233969748020172 acc: 0.7895999550819397                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.03460793197154999 acc: 0.7895999550819397                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.15687300264835358 | acc: 0.7895999550819397 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.024297451600432396 acc: 0.7895999550819397                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.019395051524043083 acc: 0.7895999550819397                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.028162319213151932 | acc: 0.7895999550819397 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.12156837433576584 acc: 0.7895999550819397                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.03516704961657524 acc: 0.7895999550819397                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.578062891960144 acc: 0.44099998474121094                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.406952142715454 acc: 0.5507999658584595                          \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0538156032562256 acc: 0.5748000144958496                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0198619365692139 | acc: 0.598800003528595 | acc_grad: [0.00692539]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0162839889526367 | acc: 0.6649999618530273 | acc_grad: [0.00542185]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.911263644695282 | acc: 0.6877999901771545 | acc_grad: [0.00438169]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9368369579315186 | acc: 0.6977999806404114 | acc_grad: [0.00315031]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6590994596481323 | acc: 0.727400004863739 | acc_grad: [0.00233446]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.560506284236908 | acc: 0.7283999919891357 | acc_grad: [0.00222969]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.48343023657798767 | acc: 0.740399956703186 | acc_grad: [0.00205292]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.49695104360580444 | acc: 0.7626000046730042 | acc_grad: [0.00119631]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.6816906929016113 | acc: 0.7531999945640564 | acc_grad: [0.00105385]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.5236538052558899 | acc: 0.7595999836921692 | acc_grad: [0.00082938]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.38550469279289246 | acc: 0.7753999829292297 | acc_grad: [0.00054431]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.6392331719398499 | acc: 0.7752000093460083 | acc_grad: [0.00064292]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.3170829117298126 | acc: 0.7468000054359436 | acc_grad: [-4.30766894e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.132020503282547 acc: 0.7888000011444092                          \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.07383416593074799 acc: 0.7889999747276306                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.11755728721618652 | acc: 0.7910000085830688 | acc_grad: [0.00103062]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.11846743524074554 | acc: 0.7910000085830688 | acc_grad: [2.96923747e-05]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.043875306844711304 | acc: 0.7888000011444092 | acc_grad: [-9.69208204e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0952167958021164 acc: 0.7889999747276306                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.08869282901287079 acc: 0.7906000018119812                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.14083321392536163 | acc: 0.7913999557495117 | acc_grad: [-4.30813202e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.06269730627536774 acc: 0.7913999557495117                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.0999007299542427 acc: 0.7911999821662903                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.02550002560019493 | acc: 0.7910000085830688 | acc_grad: [3.07725026e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05131181702017784 acc: 0.7910000085830688                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.06970173120498657 acc: 0.7910000085830688                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.07615411281585693 | acc: 0.7913999557495117 | acc_grad: [8.15414465e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07343751937150955 acc: 0.7910000085830688                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.08112720400094986 acc: 0.7906000018119812                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.014809545129537582 | acc: 0.7910000085830688 | acc_grad: [-4.30712333e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.016476549208164215 acc: 0.7911999821662903                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.05047468841075897 acc: 0.7907999753952026                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.03801359981298447 | acc: 0.7910000085830688 | acc_grad: [-6.46228974e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07155301421880722 acc: 0.7910000085830688                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.03868173062801361 acc: 0.7910000085830688                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.073178730905056 | acc: 0.7910000085830688 | acc_grad: [5.07776554e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10667036473751068 acc: 0.7910000085830688                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.06759162247180939 acc: 0.7910000085830688                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.057514891028404236 | acc: 0.7910000085830688 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0346747525036335 acc: 0.7910000085830688                          \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.022508664056658745 acc: 0.7910000085830688                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.027609078213572502 | acc: 0.7910000085830688 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.11971291899681091 acc: 0.7910000085830688                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.09485038369894028 acc: 0.7910000085830688                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.05374567210674286 | acc: 0.7910000085830688 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10339292138814926 acc: 0.7910000085830688                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.730320692062378 acc: 0.4373999834060669                          \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.3136112689971924 acc: 0.4875999987125397                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.2414159774780273 acc: 0.5759999752044678                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.1245503425598145 | acc: 0.6046000123023987 | acc_grad: [0.00751292]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0366787910461426 | acc: 0.6571999788284302 | acc_grad: [0.00531892]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.9423384070396423 | acc: 0.6435999870300293 | acc_grad: [0.00427]                           \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6411957144737244 | acc: 0.6891999840736389 | acc_grad: [0.00393692]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.9502944350242615 | acc: 0.717799961566925 | acc_grad: [0.00306015]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5423523783683777 | acc: 0.7253999710083008 | acc_grad: [0.00262338]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.4977881908416748 | acc: 0.7281999588012695 | acc_grad: [0.001848]                           \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5437613129615784 | acc: 0.7436000108718872 | acc_grad: [0.00197123]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.43098217248916626 | acc: 0.7491999864578247 | acc_grad: [0.00138492]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.45974045991897583 | acc: 0.751800000667572 | acc_grad: [0.00111338]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5109242796897888 | acc: 0.7665999531745911 | acc_grad: [0.00075585]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3218781054019928 | acc: 0.7615999579429626 | acc_grad: [0.00012431]                             \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.20839883387088776 | acc: 0.7623999714851379 | acc_grad: [0.00043569]                            \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.33110618591308594 | acc: 0.7572000026702881 | acc_grad: [0.00053308]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.23282240331172943 | acc: 0.7577999830245972 | acc_grad: [0.000356]                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.18459215760231018 | acc: 0.7504000067710876 | acc_grad: [9.0773289e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.19454020261764526 acc: 0.7883999943733215                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.09478984773159027 acc: 0.7833999991416931                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.01663958840072155 | acc: 0.7915999889373779 | acc_grad: [0.00149969]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.06724128127098083 | acc: 0.7889999747276306 | acc_grad: [9.75379119e-05]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.016133995726704597 | acc: 0.7919999957084656 | acc_grad: [0.00012092]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.15612101554870605 | acc: 0.7889999747276306 | acc_grad: [6.30786786e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010819216258823872 acc: 0.7911999821662903                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.017757240682840347 acc: 0.7924000024795532                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.00711212120950222 | acc: 0.79339998960495 | acc_grad: [0.00011477]                           \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.0027607015799731016 | acc: 0.7919999957084656 | acc_grad: [2.43079662e-05]                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.005192705430090427 | acc: 0.7924000024795532 | acc_grad: [8.00045637e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02322450652718544 acc: 0.7931999564170837                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.04346243664622307 acc: 0.7931999564170837                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.017885642126202583 | acc: 0.7928000092506409 | acc_grad: [1.59996289e-05]                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.01064440980553627 | acc: 0.7929999828338623 | acc_grad: [6.46128104e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.020897898823022842 acc: 0.7929999828338623                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.008974309079349041 acc: 0.7929999828338623                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.020002515986561775 | acc: 0.7931999564170837 | acc_grad: [4.1532058e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.023832835257053375 acc: 0.7928000092506409                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.007477411534637213 acc: 0.7931999564170837                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.006895011756569147 | acc: 0.7929999828338623 | acc_grad: [-2.15388261e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002859857166185975 acc: 0.7929999828338623                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.02332523837685585 acc: 0.7931999564170837                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.009455722756683826 | acc: 0.7929999828338623 | acc_grad: [9.69153184e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02552211657166481 acc: 0.7929999828338623                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.016494054347276688 acc: 0.7929999828338623                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.002181605203077197 | acc: 0.7929999828338623 | acc_grad: [-8.61424666e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.025374718010425568 acc: 0.7929999828338623                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.007490094285458326 acc: 0.7929999828338623                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7154254913330078 acc: 0.3789999783039093                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.2786118984222412 acc: 0.5175999999046326                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.321738839149475 acc: 0.5483999848365784                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.2806475162506104 | acc: 0.5601999759674072 | acc_grad: [0.00743292]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8194245100021362 | acc: 0.6711999773979187 | acc_grad: [0.00570954]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.011782169342041 | acc: 0.6541999578475952 | acc_grad: [0.00484261]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.9340066909790039 | acc: 0.6693999767303467 | acc_grad: [0.00351231]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.945451557636261 | acc: 0.7081999778747559 | acc_grad: [0.00226477]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.8573427200317383 | acc: 0.7307999730110168 | acc_grad: [0.00237492]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5302340388298035 | acc: 0.7351999878883362 | acc_grad: [0.00230246]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.5921987295150757 | acc: 0.7468000054359436 | acc_grad: [0.00182446]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.48892974853515625 | acc: 0.7483999729156494 | acc_grad: [0.00120569]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.5398845672607422 | acc: 0.7411999702453613 | acc_grad: [0.00033369]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.5505654811859131 | acc: 0.7477999925613403 | acc_grad: [0.00028615]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.38554027676582336 | acc: 0.7683999538421631 | acc_grad: [0.00063154]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.4297560453414917 | acc: 0.7651999592781067 | acc_grad: [0.00046569]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.16587761044502258 | acc: 0.7767999768257141 | acc_grad: [0.00031554]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.3683474361896515 | acc: 0.765999972820282 | acc_grad: [0.00045154]                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.14533847570419312 | acc: 0.7621999979019165 | acc_grad: [-2.81526492e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.10289579629898071 acc: 0.7859999537467957                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.023432934656739235 acc: 0.7925999760627747                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.06664551049470901 | acc: 0.7907999753952026 | acc_grad: [0.00148846]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.058713361620903015 | acc: 0.7893999814987183 | acc_grad: [0.00015169]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.0632028877735138 | acc: 0.790399968624115 | acc_grad: [6.78459039e-05]                           \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.013365499675273895 | acc: 0.7895999550819397 | acc_grad: [1.07723933e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.007679634727537632 acc: 0.7924000024795532                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.012060776352882385 acc: 0.7913999557495117                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.010799040086567402 | acc: 0.7913999557495117 | acc_grad: [6.83083901e-05]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.00565297482535243 | acc: 0.7915999889373779 | acc_grad: [-4.46136181e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01723461225628853 acc: 0.7924000024795532                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.005450107157230377 acc: 0.7924000024795532                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.008547992445528507 | acc: 0.7924000024795532 | acc_grad: [4.56933333e-05]                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.03488471359014511 | acc: 0.792199969291687 | acc_grad: [3.84587508e-06]                           \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02096351608633995 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.013252971693873405 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.030874453485012054 | acc: 0.792199969291687 | acc_grad: [-3.38554382e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.005461399909108877 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.005890128668397665 acc: 0.792199969291687                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.001954228151589632 | acc: 0.792199969291687 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005216218531131744 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.051762357354164124 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0030523440800607204 | acc: 0.792199969291687 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008840933442115784 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0037677560467272997 acc: 0.792199969291687                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.007853188551962376 | acc: 0.792199969291687 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.011849364265799522 acc: 0.792199969291687                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5456573963165283 acc: 0.43799999356269836                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.4821968078613281 acc: 0.5361999869346619                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0160151720046997 acc: 0.5787999629974365                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.258570671081543 | acc: 0.6295999884605408 | acc_grad: [0.00773969]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8300161957740784 | acc: 0.6570000052452087 | acc_grad: [0.005914]                          \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6828083395957947 | acc: 0.6825999617576599 | acc_grad: [0.00431123]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.5355736613273621 | acc: 0.7063999772071838 | acc_grad: [0.00348661]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.7854889631271362 | acc: 0.7179999947547913 | acc_grad: [0.00281154]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6342943906784058 | acc: 0.7263999581336975 | acc_grad: [0.00194677]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.650776743888855 | acc: 0.7429999709129333 | acc_grad: [0.00146046]                          \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.7366564273834229 | acc: 0.738599956035614 | acc_grad: [0.00151615]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.6037228107452393 | acc: 0.7541999816894531 | acc_grad: [0.00096985]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.4915160536766052 | acc: 0.7662000060081482 | acc_grad: [0.00078462]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.6008830070495605 | acc: 0.7565999627113342 | acc_grad: [0.00111]                            \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.24136009812355042 | acc: 0.7669999599456787 | acc_grad: [0.00036846]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.525456964969635 | acc: 0.7717999815940857 | acc_grad: [0.00028554]                          \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.2664240896701813 | acc: 0.7685999870300293 | acc_grad: [0.00045046]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.1791275441646576 | acc: 0.7770000100135803 | acc_grad: [0.00013231]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.13946016132831573 | acc: 0.7551999688148499 | acc_grad: [-4.30767353e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.1488657295703888 acc: 0.7931999564170837                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.02972998656332493 acc: 0.7913999557495117                         \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.04394478350877762 | acc: 0.7915999889373779 | acc_grad: [0.00102462]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.006933814380317926 | acc: 0.792199969291687 | acc_grad: [7.07695117e-05]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.08820302039384842 | acc: 0.7901999950408936 | acc_grad: [1.98465127e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.017185350880026817 | acc: 0.7911999821662903 | acc_grad: [6.15399617e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.12045370042324066 acc: 0.7924000024795532                          \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.01525124441832304 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.004025190602988005 | acc: 0.7924000024795532 | acc_grad: [9.84620589e-05]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.006203432101756334 | acc: 0.792199969291687 | acc_grad: [6.15532582e-07]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.030122896656394005 acc: 0.7924000024795532                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.031092572957277298 acc: 0.7925999760627747                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.004450487904250622 | acc: 0.7925999760627747 | acc_grad: [-2.20007621e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.008413570001721382 acc: 0.7928000092506409                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0030437533278018236 acc: 0.7925999760627747                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.030232690274715424 | acc: 0.7925999760627747 | acc_grad: [4.30881977e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.1239597424864769 acc: 0.7925999760627747                          \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.006118738558143377 acc: 0.7928000092506409                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.01912689581513405 | acc: 0.7928000092506409 | acc_grad: [-4.61615049e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009839667938649654 acc: 0.7928000092506409                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.00456663966178894 acc: 0.7928000092506409                          \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.0406445674598217 | acc: 0.7928000092506409 | acc_grad: [4.46227881e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016005445271730423 acc: 0.7928000092506409                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.07171140611171722 acc: 0.7928000092506409                          \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.022264858707785606 | acc: 0.7928000092506409 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010909686796367168 acc: 0.7928000092506409                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.08450503647327423 acc: 0.7928000092506409                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.013896962627768517 | acc: 0.7928000092506409 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.018180765211582184 acc: 0.7928000092506409                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    silu_const_var_loss_arr, silu_const_var_acc_validation, silu_const_var_model, silu_const_var_optimiser = run_test(lambda x: ConstVarInitialisation(x, 1.6766, normal=True), activation=torch.nn.SiLU)\n",
    "    silu_const_var_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"silu_const_var_loss_arr\", silu_const_var_loss_arr, i)\n",
    "    save_arr_local(\"silu_const_var_acc_validation\", silu_const_var_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"silu_const_var_model\", silu_const_var_model, i)\n",
    "    save_net_local(\"silu_const_var_optimiser\", silu_const_var_optimiser, i)\n",
    "\n",
    "    del silu_const_var_loss_arr\n",
    "    del silu_const_var_acc_validation\n",
    "    del silu_const_var_model\n",
    "    del silu_const_var_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GJUY2rh2lr-1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7cbtE1re6nyz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1NSY1__j6n1L",
    "outputId": "35160a6c-b9eb-4dc6-f42a-4ae093570107",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.3033862113952637 acc: 0.09679999947547913                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 2.2967543601989746 acc: 0.2133999913930893                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 2.0559635162353516 acc: 0.24479998648166656                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.633154273033142 | acc: 0.3211999833583832 | acc_grad: [0.00918262]                          \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.6973848342895508 | acc: 0.35420000553131104 | acc_grad: [0.00751462]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.5837205648422241 | acc: 0.3840000033378601 | acc_grad: [0.00696385]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.354397177696228 | acc: 0.4657999873161316 | acc_grad: [0.00594631]                          \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.2317569255828857 | acc: 0.5009999871253967 | acc_grad: [0.00678169]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 1.3739430904388428 | acc: 0.5365999937057495 | acc_grad: [0.00586569]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 1.3959945440292358 | acc: 0.5703999996185303 | acc_grad: [0.00400646]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 1.1674680709838867 | acc: 0.5791999697685242 | acc_grad: [0.00382892]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 1.0853503942489624 | acc: 0.6197999715805054 | acc_grad: [0.00329985]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.8009033799171448 | acc: 0.6434000134468079 | acc_grad: [0.00356754]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.8561630249023438 | acc: 0.6503999829292297 | acc_grad: [0.00313108]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.8711219429969788 | acc: 0.6764000058174133 | acc_grad: [0.00178846]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.8098722696304321 | acc: 0.6855999827384949 | acc_grad: [0.00243677]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.7371469736099243 | acc: 0.7197999954223633 | acc_grad: [0.00267738]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.656076192855835 | acc: 0.7131999731063843 | acc_grad: [0.001432]                           \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.5443784594535828 | acc: 0.715399980545044 | acc_grad: [0.00159446]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.7606860399246216 | acc: 0.7301999926567078 | acc_grad: [0.00131123]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.7161940932273865 | acc: 0.7152000069618225 | acc_grad: [0.00079615]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.8899651169776917 | acc: 0.7411999702453613 | acc_grad: [0.00062077]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.5086266994476318 | acc: 0.7453999519348145 | acc_grad: [0.00085723]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.5029776692390442 | acc: 0.7461999654769897 | acc_grad: [0.00118046]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.5770898461341858 | acc: 0.7436000108718872 | acc_grad: [0.00049508]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.5692828297615051 | acc: 0.7662000060081482 | acc_grad: [0.00060016]                             \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.735991358757019 | acc: 0.7752000093460083 | acc_grad: [0.00121323]                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.457109659910202 | acc: 0.7669999599456787 | acc_grad: [0.00133169]                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.4892471730709076 | acc: 0.7737999558448792 | acc_grad: [0.00108185]                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.5083713531494141 | acc: 0.7815999984741211 | acc_grad: [0.00039077]                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.5836005806922913 | acc: 0.776199996471405 | acc_grad: [0.00042585]                          \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.3791199326515198 | acc: 0.7807999849319458 | acc_grad: [0.00015462]                          \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.36343610286712646 | acc: 0.7577999830245972 | acc_grad: [-6.84603361e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.2838626801967621 acc: 0.8050000071525574                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.24221022427082062 acc: 0.8037999868392944                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.17131997644901276 | acc: 0.8053999543190002 | acc_grad: [0.00074723]                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.31909653544425964 | acc: 0.8046000003814697 | acc_grad: [3.49229116e-05]                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.34738147258758545 | acc: 0.8029999732971191 | acc_grad: [4.90773183e-05]                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.3005521297454834 | acc: 0.8027999997138977 | acc_grad: [7.69816912e-07]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.2807619273662567 acc: 0.8086000084877014                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.2634222209453583 acc: 0.8075999617576599                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.3016278743743896 acc: 0.09679999947547913                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 2.2995026111602783 acc: 0.13660000264644623                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 2.2518296241760254 acc: 0.1931999921798706                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 2.071014404296875 | acc: 0.26100000739097595 | acc_grad: [0.00744277]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.9119865894317627 | acc: 0.32179999351501465 | acc_grad: [0.00798769]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.6993436813354492 | acc: 0.3871999979019165 | acc_grad: [0.00751585]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.619863748550415 | acc: 0.4311999976634979 | acc_grad: [0.00634477]                          \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.233755350112915 | acc: 0.4699999988079071 | acc_grad: [0.00591185]                          \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 1.341233730316162 | acc: 0.5029999613761902 | acc_grad: [0.00564892]                          \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 1.269270420074463 | acc: 0.5579999685287476 | acc_grad: [0.00524985]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 1.2225992679595947 | acc: 0.5759999752044678 | acc_grad: [0.00459585]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.9849548935890198 | acc: 0.6123999953269958 | acc_grad: [0.00482169]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 1.1239650249481201 | acc: 0.6245999932289124 | acc_grad: [0.00434062]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.8208383321762085 | acc: 0.6549999713897705 | acc_grad: [0.00339523]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.9271814227104187 | acc: 0.6645999550819397 | acc_grad: [0.00242231]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.7856104373931885 | acc: 0.6873999834060669 | acc_grad: [0.00227108]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.7401487827301025 | acc: 0.7059999704360962 | acc_grad: [0.00211523]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.726374089717865 | acc: 0.7117999792098999 | acc_grad: [0.00229446]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.6895520687103271 | acc: 0.7067999839782715 | acc_grad: [0.00180723]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.5586637854576111 | acc: 0.7307999730110168 | acc_grad: [0.00106446]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.7202298045158386 | acc: 0.7405999898910522 | acc_grad: [0.00130308]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.7816804647445679 | acc: 0.7396000027656555 | acc_grad: [0.00129123]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.6351838707923889 | acc: 0.7671999931335449 | acc_grad: [0.00129954]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.3862507939338684 | acc: 0.7671999931335449 | acc_grad: [0.00113585]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.9733327627182007 | acc: 0.7591999769210815 | acc_grad: [0.00067738]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.6449829339981079 | acc: 0.7591999769210815 | acc_grad: [0.00051554]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.551437497138977 | acc: 0.7712000012397766 | acc_grad: [0.00050308]                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.25509074330329895 | acc: 0.7731999754905701 | acc_grad: [7.03088595e-05]                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.47753599286079407 | acc: 0.7603999972343445 | acc_grad: [-0.00012815]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.27299878001213074 acc: 0.8065999746322632                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.27378687262535095 acc: 0.8047999739646912                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.3269265294075012 | acc: 0.8033999800682068 | acc_grad: [0.00078477]                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.5464844703674316 | acc: 0.8046000003814697 | acc_grad: [-9.99941276e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.45018649101257324 acc: 0.8068000078201294                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.2161412537097931 acc: 0.8079999685287476                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.23697271943092346 | acc: 0.8123999834060669 | acc_grad: [0.00023938]                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.34009644389152527 | acc: 0.8093999624252319 | acc_grad: [0.000124]                          \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.3038603365421295 | acc: 0.8095999956130981 | acc_grad: [2.06155502e-05]                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.4394950866699219 | acc: 0.8089999556541443 | acc_grad: [3.53836096e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.2717171013355255 acc: 0.8109999895095825                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.20651133358478546 acc: 0.8091999888420105                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.2982037663459778 | acc: 0.8091999888420105 | acc_grad: [-5.23108702e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.34808510541915894 acc: 0.8091999888420105                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.302457332611084 acc: 0.10419999808073044                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 2.300023078918457 acc: 0.1581999957561493                          \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 2.278054714202881 acc: 0.18639999628067017                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 2.010632276535034 | acc: 0.3009999990463257 | acc_grad: [0.007]                               \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.8803778886795044 | acc: 0.31439998745918274 | acc_grad: [0.00780508]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.6622824668884277 | acc: 0.36079999804496765 | acc_grad: [0.00759308]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.4745230674743652 | acc: 0.4017999768257141 | acc_grad: [0.00593338]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.4246642589569092 | acc: 0.4615999758243561 | acc_grad: [0.00510954]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 1.4486699104309082 | acc: 0.4973999857902527 | acc_grad: [0.00506631]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 1.5345205068588257 | acc: 0.5406000018119812 | acc_grad: [0.00636215]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 1.3007608652114868 | acc: 0.5658000111579895 | acc_grad: [0.00541877]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 1.0429431200027466 | acc: 0.6087999939918518 | acc_grad: [0.004362]                          \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 1.201141595840454 | acc: 0.6211999654769897 | acc_grad: [0.00386]                            \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.9640421867370605 | acc: 0.6355999708175659 | acc_grad: [0.00309708]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.6027233600616455 | acc: 0.668999969959259 | acc_grad: [0.002738]                           \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.8592402338981628 | acc: 0.6797999739646912 | acc_grad: [0.00207323]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.9488887190818787 | acc: 0.6821999549865723 | acc_grad: [0.00215077]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.8517505526542664 | acc: 0.7149999737739563 | acc_grad: [0.00158508]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.8297826647758484 | acc: 0.70660001039505 | acc_grad: [0.00126308]                          \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.6679415702819824 | acc: 0.7197999954223633 | acc_grad: [0.00142031]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.7778416872024536 | acc: 0.7342000007629395 | acc_grad: [0.00124369]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.5570550560951233 | acc: 0.7389999628067017 | acc_grad: [0.00156215]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.6228322386741638 | acc: 0.7473999857902527 | acc_grad: [0.00109846]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.49790915846824646 | acc: 0.7541999816894531 | acc_grad: [0.00097015]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.5454740524291992 | acc: 0.7563999891281128 | acc_grad: [0.00104569]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.539394736289978 | acc: 0.7371999621391296 | acc_grad: [0.00074431]                          \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.6893458366394043 | acc: 0.7663999795913696 | acc_grad: [0.00094985]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.4882771670818329 | acc: 0.7797999978065491 | acc_grad: [0.00061246]                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.39828547835350037 | acc: 0.7785999774932861 | acc_grad: [0.00084262]                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.4058150053024292 | acc: 0.7856000065803528 | acc_grad: [0.00095738]                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.3535020053386688 | acc: 0.7857999801635742 | acc_grad: [0.00040754]                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.35354453325271606 | acc: 0.7917999625205994 | acc_grad: [0.00074354]                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.20514115691184998 | acc: 0.7865999937057495 | acc_grad: [0.00052261]                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.5004720687866211 | acc: 0.7879999876022339 | acc_grad: [-6.60004066e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.42740845680236816 acc: 0.8133999705314636                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.28129899501800537 acc: 0.8139999508857727                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.3833902180194855 | acc: 0.8163999915122986 | acc_grad: [0.00134908]                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.30430135130882263 | acc: 0.8154000043869019 | acc_grad: [0.00014169]                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.28345030546188354 | acc: 0.8167999982833862 | acc_grad: [7.49242306e-05]                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.13441255688667297 | acc: 0.8197999596595764 | acc_grad: [0.00011385]                            \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.11788740009069443 | acc: 0.8139999508857727 | acc_grad: [-9.53894395e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.19548194110393524 acc: 0.814799964427948                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.3030788898468018 acc: 0.11559999734163284                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 2.2990360260009766 acc: 0.16040000319480896                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 2.2334249019622803 acc: 0.2037999927997589                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 2.0246033668518066 | acc: 0.2409999966621399 | acc_grad: [0.00664415]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.9185670614242554 | acc: 0.3527999818325043 | acc_grad: [0.00676708]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.4864451885223389 | acc: 0.37139999866485596 | acc_grad: [0.00710923]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.7066304683685303 | acc: 0.42479997873306274 | acc_grad: [0.00726662]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.4941519498825073 | acc: 0.4729999899864197 | acc_grad: [0.00605538]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 1.4372035264968872 | acc: 0.5299999713897705 | acc_grad: [0.00599785]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 1.2769575119018555 | acc: 0.5573999881744385 | acc_grad: [0.00502615]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 1.3318780660629272 | acc: 0.569599986076355 | acc_grad: [0.00433415]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 1.4131274223327637 | acc: 0.6064000129699707 | acc_grad: [0.00363323]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.8519624471664429 | acc: 0.6481999754905701 | acc_grad: [0.00303831]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 1.0642386674880981 | acc: 0.6595999598503113 | acc_grad: [0.00346461]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 1.120469570159912 | acc: 0.6455999612808228 | acc_grad: [0.00209585]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.8356372117996216 | acc: 0.6931999921798706 | acc_grad: [0.00219523]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 1.0453810691833496 | acc: 0.6887999773025513 | acc_grad: [0.00262323]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.9645260572433472 | acc: 0.692799985408783 | acc_grad: [0.00134908]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.9047055244445801 | acc: 0.7175999879837036 | acc_grad: [0.00134308]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.6982957720756531 | acc: 0.7317999601364136 | acc_grad: [0.00166323]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.8366119861602783 | acc: 0.7261999845504761 | acc_grad: [0.00130969]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.4833337068557739 | acc: 0.7483999729156494 | acc_grad: [0.00084692]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.5305450558662415 | acc: 0.7495999932289124 | acc_grad: [0.00113431]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.5512715578079224 | acc: 0.7505999803543091 | acc_grad: [0.00142754]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.7168251872062683 | acc: 0.7609999775886536 | acc_grad: [0.00040369]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.6236943006515503 | acc: 0.7667999863624573 | acc_grad: [0.000766]                           \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.7184396982192993 | acc: 0.7730000019073486 | acc_grad: [0.00084677]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.5471388697624207 | acc: 0.7734000086784363 | acc_grad: [0.00045231]                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.39681366086006165 | acc: 0.751800000667572 | acc_grad: [4.61538938e-05]                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.6298438310623169 | acc: 0.7657999992370605 | acc_grad: [0.00015062]                             \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.47284960746765137 | acc: 0.7787999510765076 | acc_grad: [0.00058723]                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.49713170528411865 | acc: 0.7777999639511108 | acc_grad: [0.00044769]                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.40476855635643005 | acc: 0.7874000072479248 | acc_grad: [0.00019877]                            \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.4975186884403229 | acc: 0.7888000011444092 | acc_grad: [0.00050385]                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.41021695733070374 | acc: 0.8014000058174133 | acc_grad: [0.000626]                          \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.4584752023220062 | acc: 0.79339998960495 | acc_grad: [0.00022108]                           \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.31966307759284973 | acc: 0.7870000004768372 | acc_grad: [0.00011354]                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.393995463848114 | acc: 0.7763999700546265 | acc_grad: [-1.32311307e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.4608326554298401 acc: 0.817799985408783                          \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.06008823961019516 acc: 0.8123999834060669                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.2901107966899872 | acc: 0.8155999779701233 | acc_grad: [0.00057585]                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.3042831420898438 acc: 0.09519999474287033                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 2.3012168407440186 acc: 0.15199999511241913                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 2.278959274291992 acc: 0.20919999480247498                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 2.0015640258789062 | acc: 0.24979999661445618 | acc_grad: [0.00671754]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.7845653295516968 | acc: 0.2963999807834625 | acc_grad: [0.00760446]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.6665838956832886 | acc: 0.36399999260902405 | acc_grad: [0.00738569]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.5803625583648682 | acc: 0.42239999771118164 | acc_grad: [0.00661492]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 1.297831416130066 | acc: 0.48259997367858887 | acc_grad: [0.00690846]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 1.657370924949646 | acc: 0.5069999694824219 | acc_grad: [0.00631877]                          \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 1.3634419441223145 | acc: 0.5288000106811523 | acc_grad: [0.00570585]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 1.3390783071517944 | acc: 0.553600013256073 | acc_grad: [0.00385892]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 1.17890202999115 | acc: 0.5925999879837036 | acc_grad: [0.00287846]                          \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 1.140561580657959 | acc: 0.6092000007629395 | acc_grad: [0.00386123]                         \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.8949983716011047 | acc: 0.6597999930381775 | acc_grad: [0.00403169]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.9698865413665771 | acc: 0.667199969291687 | acc_grad: [0.00290154]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.5979664921760559 | acc: 0.676800012588501 | acc_grad: [0.00270569]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 1.1806193590164185 | acc: 0.7049999833106995 | acc_grad: [0.00213985]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.86602383852005 | acc: 0.717799961566925 | acc_grad: [0.00190723]                           \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.7328999638557434 | acc: 0.7269999980926514 | acc_grad: [0.00176985]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.7351878881454468 | acc: 0.7039999961853027 | acc_grad: [0.00100169]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.8686420321464539 | acc: 0.7409999966621399 | acc_grad: [0.001148]                           \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.7624627351760864 | acc: 0.7383999824523926 | acc_grad: [0.00147723]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.5103994011878967 | acc: 0.7489999532699585 | acc_grad: [0.00103092]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.5066618323326111 | acc: 0.7361999750137329 | acc_grad: [-0.00013308]                           \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.3803081214427948 acc: 0.7795999646186829                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.5299979448318481 acc: 0.7827999591827393                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.5839374661445618 | acc: 0.7821999788284302 | acc_grad: [0.00185831]                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.46600982546806335 | acc: 0.7827999591827393 | acc_grad: [0.00012508]                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.2429027110338211 | acc: 0.7817999720573425 | acc_grad: [7.98462904e-05]                         \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.6289626359939575 | acc: 0.7803999781608582 | acc_grad: [7.55385711e-05]                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.5337643623352051 | acc: 0.7851999998092651 | acc_grad: [5.60007646e-05]                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.5112031102180481 | acc: 0.7875999808311462 | acc_grad: [0.00017923]                            \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.5492342114448547 | acc: 0.7843999862670898 | acc_grad: [0.00022415]                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.5801146030426025 | acc: 0.7856000065803528 | acc_grad: [0.00013308]                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.4329908788204193 | acc: 0.7825999855995178 | acc_grad: [-1.95378524e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.5263855457305908 acc: 0.7892000079154968                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.4029073119163513 acc: 0.7913999557495117                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.48865267634391785 | acc: 0.7910000085830688 | acc_grad: [0.00023661]                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.4339609444141388 | acc: 0.7901999950408936 | acc_grad: [5.99994109e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.4288785755634308 acc: 0.7907999753952026                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.2668975591659546 acc: 0.7907999753952026                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.7700440287590027 | acc: 0.7911999821662903 | acc_grad: [3.07689263e-05]                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.4144026041030884 | acc: 0.7897999882698059 | acc_grad: [5.99975769e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.3378702402114868 acc: 0.7899999618530273                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    relu_xavier_loss_arr, relu_xavier_acc_validation, relu_xavier_model, relu_xavier_optimiser = run_test(lambda x: XavierInitialisation(x), activation=torch.nn.ReLU)\n",
    "    relu_xavier_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"relu_xavier_const_var_loss_arr\", relu_xavier_loss_arr, i)\n",
    "    save_arr_local(\"relu_xavier_acc_validation\", relu_xavier_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"relu_xavier_model\", relu_xavier_model, i)\n",
    "    save_net_local(\"relu_xavier_model\", relu_xavier_optimiser, i)\n",
    "\n",
    "    del relu_xavier_loss_arr\n",
    "    del relu_xavier_acc_validation\n",
    "    del relu_xavier_model\n",
    "    del relu_xavier_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1B5Tcvy3moyo",
    "outputId": "a9226721-d544-43af-b562-0cf6b0786bf6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.1822665929794312 acc: 0.5525999665260315                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.1515467166900635 acc: 0.6299999952316284                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.9912800192832947 acc: 0.6935999989509583                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.7519784569740295 | acc: 0.7233999967575073 | acc_grad: [0.00797723]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.0060347318649292 | acc: 0.7235999703407288 | acc_grad: [0.00426769]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.3980664610862732 | acc: 0.7613999843597412 | acc_grad: [0.002544]                           \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.37546849250793457 | acc: 0.7635999917984009 | acc_grad: [0.00158338]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.4101008176803589 | acc: 0.7730000019073486 | acc_grad: [0.00178723]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6122859120368958 | acc: 0.7767999768257141 | acc_grad: [0.00121061]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.48173952102661133 | acc: 0.7703999876976013 | acc_grad: [0.000772]                          \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.12453656643629074 | acc: 0.7845999598503113 | acc_grad: [0.000266]                          \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.42868778109550476 | acc: 0.7757999897003174 | acc_grad: [0.00020938]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.244341179728508 | acc: 0.7827999591827393 | acc_grad: [5.53768415e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.029754692688584328 acc: 0.8027999997138977                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.0995299220085144 acc: 0.8097999691963196                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.01906805858016014 | acc: 0.812999963760376 | acc_grad: [0.00123308]                          \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.02605990506708622 | acc: 0.8115999698638916 | acc_grad: [0.00041292]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.02557569369673729 | acc: 0.8123999834060669 | acc_grad: [0.00016138]                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.023045498877763748 | acc: 0.8149999976158142 | acc_grad: [0.0001]                                \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.01813068985939026 | acc: 0.812999963760376 | acc_grad: [0.00014323]                          \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.008191579021513462 | acc: 0.8151999711990356 | acc_grad: [7.43069557e-05]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.032774917781353 | acc: 0.8139999508857727 | acc_grad: [2.46107578e-06]                           \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03438045084476471 acc: 0.814799964427948                          \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.007876348681747913 acc: 0.8137999773025513                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.0086599076166749 | acc: 0.8141999840736389 | acc_grad: [-5.07692649e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010296008549630642 acc: 0.8139999508857727                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.005038133822381496 acc: 0.8139999508857727                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.0146402046084404 | acc: 0.8141999840736389 | acc_grad: [1.89234202e-05]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.00762451346963644 | acc: 0.8141999840736389 | acc_grad: [9.23147568e-06]                          \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.013217120431363583 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.007959414273500443 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.008982913568615913 | acc: 0.8141999840736389 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016572600230574608 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.0058244820684194565 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.010278129950165749 | acc: 0.8141999840736389 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008371223695576191 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.01937141641974449 acc: 0.8141999840736389                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.014065364375710487 | acc: 0.8141999840736389 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.008294575847685337 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.010426403023302555 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.011370236985385418 | acc: 0.8141999840736389 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008861474692821503 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.025485312566161156 acc: 0.8141999840736389                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.013722444884479046 | acc: 0.8141999840736389 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.007996308617293835 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.027304349467158318 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.008318542502820492 | acc: 0.8141999840736389 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012007557787001133 acc: 0.8141999840736389                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.017113549634814262 acc: 0.8141999840736389                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.3104749917984009 acc: 0.5737999677658081                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.0070418119430542 acc: 0.6534000039100647                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.9857308864593506 acc: 0.701200008392334                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.8343542218208313 | acc: 0.7202000021934509 | acc_grad: [0.00656323]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.602036714553833 | acc: 0.7325999736785889 | acc_grad: [0.00359108]                          \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.4567578136920929 | acc: 0.7716000080108643 | acc_grad: [0.002776]                           \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.4707643687725067 | acc: 0.7735999822616577 | acc_grad: [0.00236631]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.43958741426467896 | acc: 0.7712000012397766 | acc_grad: [0.00134785]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.29540425539016724 | acc: 0.7813999652862549 | acc_grad: [0.00045369]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.3179292380809784 | acc: 0.7773999571800232 | acc_grad: [0.00031615]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.22819362580776215 | acc: 0.7649999856948853 | acc_grad: [0.00018215]                            \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.23292195796966553 | acc: 0.7823999524116516 | acc_grad: [0.00021877]                            \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.17132045328617096 | acc: 0.7865999937057495 | acc_grad: [0.00053831]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.07496532052755356 | acc: 0.8003999590873718 | acc_grad: [0.00081985]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.09037095308303833 | acc: 0.7787999510765076 | acc_grad: [5.46153234e-05]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.23219984769821167 | acc: 0.7705999612808228 | acc_grad: [-0.00016154]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05625166371464729 acc: 0.8155999779701233                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.1137717217206955 acc: 0.8139999508857727                          \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.011366542428731918 | acc: 0.8172000050544739 | acc_grad: [0.00095462]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.004699329845607281 | acc: 0.8145999908447266 | acc_grad: [7.36929362e-05]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.010347572155296803 | acc: 0.8197999596595764 | acc_grad: [0.00015431]                            \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.0052255867049098015 | acc: 0.8187999725341797 | acc_grad: [0.00014692]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.001998934894800186 | acc: 0.818399965763092 | acc_grad: [7.72306552e-05]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.009322259575128555 | acc: 0.8163999915122986 | acc_grad: [-2.04619536e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.007942146621644497 acc: 0.8167999982833862                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.007074841298162937 acc: 0.8179999589920044                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.007941844873130322 | acc: 0.8172000050544739 | acc_grad: [-1.46145546e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004901260603219271 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.003395709441974759 acc: 0.8173999786376953                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.006949640344828367 | acc: 0.8172000050544739 | acc_grad: [-1.56918856e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006346133071929216 acc: 0.8172000050544739                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0035088621079921722 acc: 0.8172000050544739                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.00453514838591218 | acc: 0.8172000050544739 | acc_grad: [-2.3072041e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0043775602243840694 acc: 0.8172000050544739                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.009427960962057114 acc: 0.8172000050544739                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0038706434424966574 | acc: 0.8169999718666077 | acc_grad: [-3.53904871e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006064392626285553 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0033084654714912176 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.003233750117942691 | acc: 0.8169999718666077 | acc_grad: [-8.77068593e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.008784318342804909 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.007168306969106197 acc: 0.8169999718666077                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.014632289297878742 | acc: 0.8169999718666077 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0034696394577622414 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.008519637398421764 acc: 0.8169999718666077                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.002123739803209901 | acc: 0.8169999718666077 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0030545529443770647 acc: 0.8169999718666077                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.01417570374906063 acc: 0.8169999718666077                          \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0066868942230939865 | acc: 0.8169999718666077 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006049932911992073 acc: 0.8169999718666077                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.1197614669799805 acc: 0.5497999787330627                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 0.8430613875389099 acc: 0.6351999640464783                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.8321057558059692 acc: 0.6805999875068665                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.873728334903717 | acc: 0.7227999567985535 | acc_grad: [0.00730538]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.4040093421936035 | acc: 0.7303999662399292 | acc_grad: [0.004594]                          \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.808750569820404 | acc: 0.7576000094413757 | acc_grad: [0.00279354]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.5938106775283813 | acc: 0.7608000040054321 | acc_grad: [0.00214231]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6485819220542908 | acc: 0.7601999640464783 | acc_grad: [0.00154692]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.2768615484237671 | acc: 0.7795999646186829 | acc_grad: [0.00077954]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.2584962546825409 | acc: 0.7815999984741211 | acc_grad: [0.00081769]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.20682473480701447 | acc: 0.772599995136261 | acc_grad: [0.00068723]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.23233412206172943 | acc: 0.7892000079154968 | acc_grad: [0.00021092]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.09190519154071808 | acc: 0.7899999618530273 | acc_grad: [0.00011031]                            \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.13305820524692535 | acc: 0.79339998960495 | acc_grad: [0.00042446]                              \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.13887052237987518 | acc: 0.7915999889373779 | acc_grad: [8.58463691e-05]                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.23804457485675812 | acc: 0.7895999550819397 | acc_grad: [2.58460412e-05]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.06461635231971741 | acc: 0.7851999998092651 | acc_grad: [-4.80004457e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03725014999508858 acc: 0.8104000091552734                         \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.009578714147210121 acc: 0.8119999766349792                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.04700121283531189 | acc: 0.8151999711990356 | acc_grad: [0.00088769]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.006884357891976833 | acc: 0.8159999847412109 | acc_grad: [0.00023646]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.011121155694127083 | acc: 0.8161999583244324 | acc_grad: [0.00017092]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.008631354197859764 | acc: 0.8149999976158142 | acc_grad: [5.89234554e-05]                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.011753539554774761 | acc: 0.8161999583244324 | acc_grad: [4.46150853e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.006998873315751553 | acc: 0.817799985408783 | acc_grad: [8.83075824e-05]                          \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.001797712524421513 | acc: 0.8172000050544739 | acc_grad: [4.87696666e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.005042692180722952 | acc: 0.818399965763092 | acc_grad: [6.90774276e-05]                          \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.00586362648755312 | acc: 0.8181999921798706 | acc_grad: [7.03073006e-05]                          \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.005272412206977606 | acc: 0.8169999718666077 | acc_grad: [-3.84642528e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.002734370529651642 acc: 0.81659996509552                           \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.004245257005095482 acc: 0.8161999583244324                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0031157846096903086 | acc: 0.8159999847412109 | acc_grad: [-6.64614714e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0024908154737204313 acc: 0.8161999583244324                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.004283547401428223 acc: 0.8163999915122986                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.0021611503325402737 | acc: 0.8161999583244324 | acc_grad: [-1.41535355e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005530592054128647 acc: 0.8163999915122986                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.00198249239474535 acc: 0.8163999915122986                          \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0032219067215919495 | acc: 0.8163999915122986 | acc_grad: [2.61581861e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.003964809235185385 acc: 0.8163999915122986                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.004432727117091417 acc: 0.8163999915122986                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.006728841923177242 | acc: 0.8163999915122986 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008576766587793827 acc: 0.8163999915122986                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.0017007550923153758 acc: 0.8163999915122986                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.005103894043713808 | acc: 0.8163999915122986 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004717879928648472 acc: 0.8163999915122986                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0024248689878731966 acc: 0.8163999915122986                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.1685280799865723 acc: 0.5485999584197998                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 0.9866474866867065 acc: 0.6043999791145325                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0078749656677246 acc: 0.6904000043869019                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.705125629901886 | acc: 0.7269999980926514 | acc_grad: [0.00677369]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.6973370313644409 | acc: 0.733199954032898 | acc_grad: [0.00452215]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6410093903541565 | acc: 0.7513999938964844 | acc_grad: [0.002958]                           \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.3894875645637512 | acc: 0.7615999579429626 | acc_grad: [0.00187631]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.4747105538845062 | acc: 0.7673999667167664 | acc_grad: [0.00134908]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.5190575122833252 | acc: 0.7849999666213989 | acc_grad: [0.00095954]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5774556398391724 | acc: 0.7653999924659729 | acc_grad: [0.00051646]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.3006836175918579 | acc: 0.7843999862670898 | acc_grad: [0.00047415]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.16029874980449677 | acc: 0.7913999557495117 | acc_grad: [0.00034831]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.11597996205091476 | acc: 0.7817999720573425 | acc_grad: [0.00062415]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.17700855433940887 | acc: 0.7901999950408936 | acc_grad: [0.00035015]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.04906756430864334 | acc: 0.7929999828338623 | acc_grad: [-1.76913005e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.050286635756492615 acc: 0.8131999969482422                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.04397454857826233 acc: 0.8136000037193298                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.015404514968395233 | acc: 0.81659996509552 | acc_grad: [0.00114938]                          \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.024875616654753685 | acc: 0.8193999528884888 | acc_grad: [0.00023108]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.007504669483751059 | acc: 0.8175999522209167 | acc_grad: [0.00011631]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.05814259126782417 | acc: 0.8185999989509583 | acc_grad: [7.15378615e-05]                          \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.005952666513621807 | acc: 0.8201999664306641 | acc_grad: [5.81539136e-05]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.009756376035511494 | acc: 0.818399965763092 | acc_grad: [2.76930974e-05]                          \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.005207100883126259 | acc: 0.8181999921798706 | acc_grad: [-7.69170431e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0062412903644144535 acc: 0.8187999725341797                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.007593950256705284 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.006368200294673443 | acc: 0.8190000057220459 | acc_grad: [3.43087086e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.006907525938004255 | acc: 0.8191999793052673 | acc_grad: [8.92345722e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004598356317728758 acc: 0.8187999725341797                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.006333840545266867 acc: 0.818399965763092                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.0071129160933196545 | acc: 0.818399965763092 | acc_grad: [-2.56937742e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016051791608333588 acc: 0.818399965763092                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.004488195758312941 acc: 0.818399965763092                          \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0035287579521536827 | acc: 0.818399965763092 | acc_grad: [7.07745552e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002225981093943119 acc: 0.818399965763092                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.0071342806331813335 acc: 0.8185999989509583                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.0037559703923761845 | acc: 0.818399965763092 | acc_grad: [6.15532582e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.011560236103832722 acc: 0.818399965763092                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.00547609431669116 acc: 0.818399965763092                          \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.006476117763668299 | acc: 0.818399965763092 | acc_grad: [-7.2314189e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002349226037040353 acc: 0.818399965763092                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.003618264803662896 acc: 0.818399965763092                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.004929474089294672 | acc: 0.818399965763092 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005304059945046902 acc: 0.818399965763092                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.004153091926127672 acc: 0.818399965763092                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.0037395856343209743 | acc: 0.818399965763092 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006554490886628628 acc: 0.818399965763092                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.0021365347784012556 acc: 0.818399965763092                        \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.1350903511047363 acc: 0.5685999989509583                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 0.7085448503494263 acc: 0.6502000093460083                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.8292438983917236 acc: 0.6911999583244324                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.6986246109008789 | acc: 0.710599958896637 | acc_grad: [0.00689123]                         \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.640045166015625 | acc: 0.7369999885559082 | acc_grad: [0.00396815]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6380691528320312 | acc: 0.7558000087738037 | acc_grad: [0.00247739]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.46486973762512207 | acc: 0.7547999620437622 | acc_grad: [0.00188354]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6299136281013489 | acc: 0.7639999985694885 | acc_grad: [0.00146338]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.3676412105560303 | acc: 0.7721999883651733 | acc_grad: [0.00095523]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.3027002513408661 | acc: 0.7899999618530273 | acc_grad: [0.00124415]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.27629756927490234 | acc: 0.7861999869346619 | acc_grad: [0.00103338]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.16130343079566956 | acc: 0.7821999788284302 | acc_grad: [5.26153124e-05]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.13174821436405182 | acc: 0.7857999801635742 | acc_grad: [-8.92307667e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05026630312204361 acc: 0.8136000037193298                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.12333530932664871 acc: 0.8175999522209167                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.021160904318094254 | acc: 0.8181999921798706 | acc_grad: [0.000992]                          \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.04437166824936867 | acc: 0.8208000063896179 | acc_grad: [0.00026769]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.010585718788206577 | acc: 0.8241999745368958 | acc_grad: [0.00020446]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.010996363125741482 | acc: 0.8233999609947205 | acc_grad: [0.00022354]                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.011729700490832329 | acc: 0.8240000009536743 | acc_grad: [0.00010292]                            \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.0076741124503314495 | acc: 0.8247999548912048 | acc_grad: [7.52305068e-05]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.02955220453441143 | acc: 0.8233999609947205 | acc_grad: [5.01535948e-05]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.015183251351118088 | acc: 0.8245999813079834 | acc_grad: [6.76907026e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006781074684113264 acc: 0.8247999548912048                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.004277526866644621 acc: 0.8247999548912048                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.003942821640521288 | acc: 0.824999988079071 | acc_grad: [3.78456941e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.01489432156085968 | acc: 0.8251999616622925 | acc_grad: [2.83074837e-05]                          \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.014292733743786812 | acc: 0.8245999813079834 | acc_grad: [8.6155763e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.00482041435316205 acc: 0.824999988079071                           \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.00639557559043169 acc: 0.8247999548912048                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.011586572974920273 | acc: 0.824999988079071 | acc_grad: [-6.76879516e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006187230348587036 acc: 0.824999988079071                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.015142550691962242 acc: 0.824999988079071                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.013658595271408558 | acc: 0.8251999616622925 | acc_grad: [1.06148078e-05]                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.008308001793920994 | acc: 0.8253999948501587 | acc_grad: [9.99968786e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.004056188743561506 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.010200107470154762 acc: 0.8253999948501587                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.003495951648801565 | acc: 0.8251999616622925 | acc_grad: [5.69274792e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006164788734167814 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.0061955698765814304 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.008301584050059319 | acc: 0.8251999616622925 | acc_grad: [1.53459035e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006773160304874182 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.006272806320339441 acc: 0.8251999616622925                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.006662304047495127 | acc: 0.8251999616622925 | acc_grad: [-3.38517703e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008872359991073608 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.012614676728844643 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.010274515487253666 | acc: 0.8251999616622925 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.005931878462433815 acc: 0.8251999616622925                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.019246110692620277 acc: 0.8251999616622925                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    selu_const_var_loss_arr, selu_const_var_acc_validation, selu_const_var_model, selu_const_var_optimiser = run_test(lambda x: ConstVarInitialisation(x, 1.0000, normal=True), activation=torch.nn.SELU)\n",
    "    selu_const_var_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"selu_const_var_loss_arr\", selu_const_var_loss_arr, i)\n",
    "    save_arr_local(\"selu_const_var_acc_validation\", selu_const_var_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"selu_const_var_model\", selu_const_var_model, i)\n",
    "    save_net_local(\"selu_const_var_optimiser\", selu_const_var_optimiser, i)\n",
    "\n",
    "    del selu_const_var_loss_arr\n",
    "    del selu_const_var_acc_validation\n",
    "    del selu_const_var_model\n",
    "    del selu_const_var_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.7813655138015747 acc: 0.483599990606308                          \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.0469251871109009 acc: 0.5807999968528748                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.9005596041679382 acc: 0.6377999782562256                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.7905160188674927 | acc: 0.6588000059127808 | acc_grad: [0.00724169]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.6445820331573486 | acc: 0.6959999799728394 | acc_grad: [0.00514677]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.7522552013397217 | acc: 0.6771999597549438 | acc_grad: [0.00326031]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.7467121481895447 | acc: 0.7027999758720398 | acc_grad: [0.00220031]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.45383191108703613 | acc: 0.7437999844551086 | acc_grad: [0.00191785]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.4298092722892761 | acc: 0.7545999884605408 | acc_grad: [0.00222369]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5727920532226562 | acc: 0.7475999593734741 | acc_grad: [0.00137631]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.2424347996711731 | acc: 0.7581999897956848 | acc_grad: [0.00080231]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.23981495201587677 | acc: 0.7576000094413757 | acc_grad: [0.00050739]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.44106048345565796 | acc: 0.7698000073432922 | acc_grad: [0.00050954]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.2090703547000885 | acc: 0.7623999714851379 | acc_grad: [0.00032185]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3379552662372589 | acc: 0.722599983215332 | acc_grad: [-7.73846645e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.1683221012353897 acc: 0.79339998960495                           \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.17564056813716888 acc: 0.7973999977111816                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.06794076412916183 | acc: 0.7960000038146973 | acc_grad: [0.00126877]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.09678742289543152 | acc: 0.7979999780654907 | acc_grad: [0.00019477]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.01895175687968731 | acc: 0.7983999848365784 | acc_grad: [9.553758e-05]                           \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.10221489518880844 | acc: 0.7971999645233154 | acc_grad: [3.23048005e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.013691389933228493 acc: 0.7979999780654907                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.018969371914863586 acc: 0.7978000044822693                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.04338746890425682 | acc: 0.7981999516487122 | acc_grad: [3.30775976e-05]                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.010368561372160912 | acc: 0.7985999584197998 | acc_grad: [3.50762331e-05]                        \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.01641872525215149 | acc: 0.7985999584197998 | acc_grad: [4.35378918e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.015410070307552814 | acc: 0.7989999651908875 | acc_grad: [2.59999587e-05]                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.01036576833575964 | acc: 0.7981999516487122 | acc_grad: [1.30768464e-05]                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.060292162001132965 | acc: 0.7983999848365784 | acc_grad: [5.38472946e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009417637251317501 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.006643650587648153 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0062879594042897224 | acc: 0.7985999584197998 | acc_grad: [-1.23086342e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.017670340836048126 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.02370472066104412 acc: 0.7985999584197998                          \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.006535780616104603 | acc: 0.7985999584197998 | acc_grad: [1.53825833e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009156927466392517 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.02237885259091854 acc: 0.7985999584197998                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.01621619425714016 | acc: 0.7985999584197998 | acc_grad: [-9.23230098e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.016553286463022232 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.027228472754359245 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.03880109637975693 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010696126148104668 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.018930191174149513 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.017076091840863228 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.015377677045762539 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.012767420150339603 acc: 0.7985999584197998                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.006865441333502531 | acc: 0.7985999584197998 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.2140312194824219 acc: 0.515999972820282                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.299077033996582 acc: 0.5654000043869019                          \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0844073295593262 acc: 0.6481999754905701                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9752229452133179 | acc: 0.6747999787330627 | acc_grad: [0.00718831]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8772101998329163 | acc: 0.7148000001907349 | acc_grad: [0.00519569]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.3167848289012909 | acc: 0.7167999744415283 | acc_grad: [0.00340646]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6329988241195679 | acc: 0.7233999967575073 | acc_grad: [0.00223246]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.608629047870636 | acc: 0.7383999824523926 | acc_grad: [0.00155661]                          \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6265867352485657 | acc: 0.7461999654769897 | acc_grad: [0.00107923]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7987382411956787 | acc: 0.7731999754905701 | acc_grad: [0.00126692]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.46692052483558655 | acc: 0.7507999539375305 | acc_grad: [0.001172]                          \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.44026434421539307 | acc: 0.7621999979019165 | acc_grad: [0.00055892]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.2962953448295593 | acc: 0.740399956703186 | acc_grad: [-0.00013938]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.17039071023464203 acc: 0.7888000011444092                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.24400991201400757 acc: 0.7935999631881714                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.1369299739599228 | acc: 0.7983999848365784 | acc_grad: [0.00193615]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.05145415663719177 | acc: 0.7975999712944031 | acc_grad: [0.00051846]                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.016461407765746117 | acc: 0.7981999516487122 | acc_grad: [4.36918552e-05]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.03913368284702301 | acc: 0.7953999638557434 | acc_grad: [-1.49241319e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.021556708961725235 acc: 0.7979999780654907                        \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.044718362390995026 acc: 0.7983999848365784                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.0229396540671587 | acc: 0.7975999712944031 | acc_grad: [2.98461547e-05]                          \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.025629764422774315 | acc: 0.7979999780654907 | acc_grad: [1.99968998e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01790410280227661 acc: 0.7983999848365784                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.011464809998869896 acc: 0.7978000044822693                        \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.02290605753660202 | acc: 0.7981999516487122 | acc_grad: [-1.63075557e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.02293972671031952 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.03374071419239044 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.036698441952466965 | acc: 0.7981999516487122 | acc_grad: [1.15369375e-05]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.10648408532142639 | acc: 0.7981999516487122 | acc_grad: [8.61424666e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.028420276939868927 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.09255947172641754 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.025175301358103752 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.014687921851873398 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.04282610863447189 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.008334940299391747 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05231985077261925 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.05009238421916962 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.13683302700519562 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01667376235127449 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.01750890165567398 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.04057939723134041 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.017465999349951744 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.024344177916646004 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.04736330360174179 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03710375353693962 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.052003100514411926 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.03143119812011719 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.010822406969964504 acc: 0.7981999516487122                        \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.06316034495830536 acc: 0.7981999516487122                         \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.1357119232416153 | acc: 0.7981999516487122 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.8189946413040161 acc: 0.49459999799728394                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.1818581819534302 acc: 0.5703999996185303                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.1125496625900269 acc: 0.6139999628067017                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 0.9764156341552734 | acc: 0.6534000039100647 | acc_grad: [0.00702985]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.8676329851150513 | acc: 0.6895999908447266 | acc_grad: [0.00523508]                        \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6003707647323608 | acc: 0.7093999981880188 | acc_grad: [0.00382492]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6283190846443176 | acc: 0.740399956703186 | acc_grad: [0.00279692]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.6369054317474365 | acc: 0.7251999974250793 | acc_grad: [0.00228754]                         \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.3925105035305023 | acc: 0.7608000040054321 | acc_grad: [0.00184677]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7263986468315125 | acc: 0.7279999852180481 | acc_grad: [0.00095923]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.3126065135002136 | acc: 0.762999951839447 | acc_grad: [0.00103323]                          \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.504057765007019 | acc: 0.7594000101089478 | acc_grad: [0.00089154]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.15957799553871155 | acc: 0.7623999714851379 | acc_grad: [0.00092985]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.4018355906009674 | acc: 0.7665999531745911 | acc_grad: [0.00025877]                         \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.3278442919254303 | acc: 0.7709999680519104 | acc_grad: [0.00048046]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.18709681928157806 | acc: 0.7663999795913696 | acc_grad: [0.00015446]                           \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.16985514760017395 | acc: 0.7799999713897705 | acc_grad: [0.00018292]                            \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.20047608017921448 | acc: 0.7825999855995178 | acc_grad: [0.00059031]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.19504550099372864 | acc: 0.7823999524116516 | acc_grad: [0.00050846]                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.10837104171514511 | acc: 0.7791999578475952 | acc_grad: [0.00050231]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.191433846950531 | acc: 0.7739999890327454 | acc_grad: [6.83079316e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.09257522970438004 | acc: 0.7737999558448792 | acc_grad: [-4.50767921e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.06297966092824936 acc: 0.8046000003814697                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.010357916355133057 acc: 0.8082000017166138                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.00730076152831316 | acc: 0.8079999685287476 | acc_grad: [0.00087723]                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.008547184988856316 | acc: 0.8107999563217163 | acc_grad: [0.00021585]                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.004951523616909981 | acc: 0.8123999834060669 | acc_grad: [0.00011738]                         \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.008324436843395233 | acc: 0.8093999624252319 | acc_grad: [4.76926565e-05]                        \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.003086534095928073 | acc: 0.8100000023841858 | acc_grad: [-5.53878454e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0032712840475142 acc: 0.8100000023841858                          \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.0012759212404489517 acc: 0.8093999624252319                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.0017804980743676424 | acc: 0.8101999759674072 | acc_grad: [-1.99996508e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.003339177230373025 acc: 0.8097999691963196                        \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.003802353050559759 acc: 0.8101999759674072                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.003181874519214034 | acc: 0.8097999691963196 | acc_grad: [1.12309823e-05]                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.0020925861317664385 | acc: 0.8097999691963196 | acc_grad: [3.38458098e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.003746014554053545 acc: 0.8093999624252319                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.002313188510015607 acc: 0.8100000023841858                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.0025985727552324533 | acc: 0.8100000023841858 | acc_grad: [-7.23036436e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.002545018680393696 acc: 0.8100000023841858                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.008890830911695957 acc: 0.8095999956130981                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.0010226276936009526 | acc: 0.8095999956130981 | acc_grad: [-3.6920034e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0020467194262892008 acc: 0.8095999956130981                        \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.005211436655372381 acc: 0.8097999691963196                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.003571267705410719 | acc: 0.8097999691963196 | acc_grad: [6.13836142e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0017376510659232736 acc: 0.8097999691963196                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.00281767756678164 acc: 0.8097999691963196                          \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.5200318098068237 acc: 0.462799996137619                          \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.0666815042495728 acc: 0.5831999778747559                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 0.8924543857574463 acc: 0.6261999607086182                        \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0717335939407349 | acc: 0.6833999752998352 | acc_grad: [0.00796262]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.909377932548523 | acc: 0.69159996509552 | acc_grad: [0.00412292]                           \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 0.6183695793151855 | acc: 0.7019999623298645 | acc_grad: [0.00315677]                        \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.6392840147018433 | acc: 0.7391999959945679 | acc_grad: [0.00241462]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.659919798374176 | acc: 0.7355999946594238 | acc_grad: [0.00175385]                          \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.7920057773590088 | acc: 0.7197999954223633 | acc_grad: [0.00154139]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.5039730668067932 | acc: 0.7597999572753906 | acc_grad: [0.00139508]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.3209027349948883 | acc: 0.7523999810218811 | acc_grad: [0.00134569]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.4006009101867676 | acc: 0.7486000061035156 | acc_grad: [0.00070431]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.29000869393348694 | acc: 0.7513999938964844 | acc_grad: [-2.50766369e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.20957528054714203 acc: 0.7853999733924866                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.182750403881073 acc: 0.7906000018119812                          \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.10187903046607971 | acc: 0.7915999889373779 | acc_grad: [0.00140554]                         \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.08470620214939117 | acc: 0.7943999767303467 | acc_grad: [0.00024446]                         \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.03480246663093567 | acc: 0.7935999631881714 | acc_grad: [0.00013292]                             \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.01779528707265854 | acc: 0.79339998960495 | acc_grad: [6.27692388e-05]                          \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.059505872428417206 | acc: 0.7901999950408936 | acc_grad: [-9.69185279e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.013769383542239666 acc: 0.7931999564170837                        \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.034313708543777466 acc: 0.7929999828338623                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.02188551425933838 | acc: 0.7929999828338623 | acc_grad: [5.15375688e-05]                         \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.026190880686044693 | acc: 0.793999969959259 | acc_grad: [1.72303732e-05]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.019719863310456276 | acc: 0.7943999767303467 | acc_grad: [5.93848412e-05]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.013402360491454601 | acc: 0.7929999828338623 | acc_grad: [2.6153647e-05]                         \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.00803693663328886 | acc: 0.7935999631881714 | acc_grad: [4.15361845e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.008293837308883667 acc: 0.7935999631881714                        \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.023000754415988922 acc: 0.79339998960495                          \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.012232985347509384 | acc: 0.793999969959259 | acc_grad: [7.38515304e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.07336010038852692 acc: 0.7942000031471252                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.023235075175762177 acc: 0.7942000031471252                        \n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.015734808519482613 | acc: 0.793999969959259 | acc_grad: [8.15345691e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.03703968971967697 acc: 0.793999969959259                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.020014647394418716 acc: 0.7942000031471252                        \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.01796291582286358 | acc: 0.7942000031471252 | acc_grad: [4.61601294e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.009849880822002888 acc: 0.7942000031471252                        \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.017913492396473885 acc: 0.793999969959259                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.02758333832025528 | acc: 0.7942000031471252 | acc_grad: [6.15028235e-07]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01955612376332283 acc: 0.7942000031471252                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.01319124922156334 acc: 0.7942000031471252                         \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.014913416467607021 | acc: 0.7942000031471252 | acc_grad: [7.84745583e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01198760885745287 acc: 0.7942000031471252                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.01842750608921051 acc: 0.7942000031471252                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.029552791267633438 | acc: 0.7942000031471252 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.03412696719169617 acc: 0.7942000031471252                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.013881954364478588 acc: 0.7942000031471252                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.02206101454794407 | acc: 0.7942000031471252 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.05071072652935982 acc: 0.7942000031471252                         \n",
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 1.2958614826202393 acc: 0.5019999742507935                         \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 1.2411326169967651 acc: 0.5917999744415283                         \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.0243797302246094 acc: 0.661799967288971                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.0106816291809082 | acc: 0.675599992275238 | acc_grad: [0.006702]                           \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 0.821281909942627 | acc: 0.7095999717712402 | acc_grad: [0.00440154]                         \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.0396827459335327 | acc: 0.715999960899353 | acc_grad: [0.00293154]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 0.588262677192688 | acc: 0.7279999852180481 | acc_grad: [0.00211646]                         \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.685728132724762 | acc: 0.7667999863624573 | acc_grad: [0.00191723]                          \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.6200814247131348 | acc: 0.7635999917984009 | acc_grad: [0.00178969]                         \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.7491648197174072 | acc: 0.7554000020027161 | acc_grad: [0.00147154]                         \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.49205708503723145 | acc: 0.7703999876976013 | acc_grad: [0.00031108]                        \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.3402585983276367 | acc: 0.7635999917984009 | acc_grad: [0.00048046]                         \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.26057499647140503 | acc: 0.7637999653816223 | acc_grad: [0.00065831]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.40993553400039673 | acc: 0.7716000080108643 | acc_grad: [1.43069487e-05]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.28185901045799255 | acc: 0.7669999599456787 | acc_grad: [-2.09237521e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.16029147803783417 acc: 0.7881999611854553                        \n",
      "=================================\n",
      "Epoch: 16\n",
      "=================================\n",
      "loss: 0.06531886756420135 acc: 0.7911999821662903                        \n",
      "=================================\n",
      "Epoch: 17\n",
      "=================================\n",
      "loss: 0.013381823897361755 | acc: 0.7967999577522278 | acc_grad: [0.00066123]                        \n",
      "=================================\n",
      "Epoch: 18\n",
      "=================================\n",
      "loss: 0.04748411104083061 | acc: 0.7953999638557434 | acc_grad: [0.00024431]                         \n",
      "=================================\n",
      "Epoch: 19\n",
      "=================================\n",
      "loss: 0.01333681307733059 | acc: 0.7942000031471252 | acc_grad: [0.00024492]                         \n",
      "=================================\n",
      "Epoch: 20\n",
      "=================================\n",
      "loss: 0.008237284608185291 | acc: 0.7949999570846558 | acc_grad: [0.00010215]                        \n",
      "=================================\n",
      "Epoch: 21\n",
      "=================================\n",
      "loss: 0.010284503921866417 | acc: 0.7963999509811401 | acc_grad: [8.13843195e-05]                        \n",
      "=================================\n",
      "Epoch: 22\n",
      "=================================\n",
      "loss: 0.006714394316077232 | acc: 0.7989999651908875 | acc_grad: [9.55384053e-05]                         \n",
      "=================================\n",
      "Epoch: 23\n",
      "=================================\n",
      "loss: 0.022047635167837143 | acc: 0.7997999787330627 | acc_grad: [9.86165267e-05]                        \n",
      "=================================\n",
      "Epoch: 24\n",
      "=================================\n",
      "loss: 0.013154703192412853 | acc: 0.8007999658584595 | acc_grad: [0.00016831]                            \n",
      "=================================\n",
      "Epoch: 25\n",
      "=================================\n",
      "loss: 0.004582905676215887 | acc: 0.8011999726295471 | acc_grad: [9.47689552e-05]                         \n",
      "=================================\n",
      "Epoch: 26\n",
      "=================================\n",
      "loss: 0.005737037863582373 | acc: 0.8003999590873718 | acc_grad: [3.23084685e-06]                         \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.007584086619317532 acc: 0.8001999855041504                        \n",
      "=================================\n",
      "Epoch: 27\n",
      "=================================\n",
      "loss: 0.005716745276004076 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 28\n",
      "=================================\n",
      "loss: 0.015024064108729362 | acc: 0.7989999651908875 | acc_grad: [-7.19996599e-05]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "\n",
      "=================================\n",
      "Epoch: 29\n",
      "=================================\n",
      "loss: 0.003636769251897931 acc: 0.7989999651908875                        \n",
      "=================================\n",
      "Epoch: 30\n",
      "=================================\n",
      "loss: 0.00587594835087657 acc: 0.7996000051498413                          \n",
      "=================================\n",
      "Epoch: 31\n",
      "=================================\n",
      "loss: 0.003306110855191946 | acc: 0.7991999983787537 | acc_grad: [-1.07668913e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.012244299054145813 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 32\n",
      "=================================\n",
      "loss: 0.002078504301607609 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 33\n",
      "=================================\n",
      "loss: 0.012873711995780468 | acc: 0.7993999719619751 | acc_grad: [2.92296593e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.006023409776389599 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 34\n",
      "=================================\n",
      "loss: 0.006445534061640501 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 35\n",
      "=================================\n",
      "loss: 0.001409152988344431 | acc: 0.7991999983787537 | acc_grad: [-5.9992075e-06]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.01311805285513401 acc: 0.7991999983787537                          \n",
      "=================================\n",
      "Epoch: 36\n",
      "=================================\n",
      "loss: 0.003792756935581565 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 37\n",
      "=================================\n",
      "loss: 0.00801452063024044 | acc: 0.7991999983787537 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.025026027113199234 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 38\n",
      "=================================\n",
      "loss: 0.007962211035192013 acc: 0.7991999983787537                         \n",
      "=================================\n",
      "Epoch: 39\n",
      "=================================\n",
      "loss: 0.006021462380886078 | acc: 0.7991999983787537 | acc_grad: [0.]                        \n",
      "Decreased Learning Rate by a factor of 10\n",
      "loss: 0.0063957031816244125 acc: 0.7991999983787537                        \n",
      "=================================\n",
      "Epoch: 40\n",
      "=================================\n",
      "loss: 0.004404765088111162 acc: 0.7991999983787537                        \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    selu_const_var_loss_arr, selu_const_var_acc_validation, selu_const_var_model, selu_const_var_optimiser = run_test(lambda x: ConstVarInitialisation(x, normalising_coefficient=1.4142, normal=True), activation=torch.nn.SELU)\n",
    "    selu_const_var_model.to(\"cpu\")\n",
    "\n",
    "    save_arr_local(\"selu_he_loss_arr\", selu_const_var_loss_arr, i)\n",
    "    save_arr_local(\"selu_he_acc_validation\", selu_const_var_acc_validation, i)\n",
    "\n",
    "    save_net_local(\"selu_he_model\", selu_const_var_model, i)\n",
    "    save_net_local(\"selu_he_optimiser\", selu_const_var_optimiser, i)\n",
    "\n",
    "    del selu_const_var_loss_arr\n",
    "    del selu_const_var_acc_validation\n",
    "    del selu_const_var_model\n",
    "    del selu_const_var_optimiser\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9NFNqdE61yQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DrQpRLtVUth",
    "outputId": "76409936-cde7-40cf-f168-c7809a8dca23",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Epoch: 1\n",
      "=================================\n",
      "loss: 2.3050475120544434 acc: 0.10080000013113022                        \n",
      "=================================\n",
      "Epoch: 2\n",
      "=================================\n",
      "loss: 2.1452205181121826 | acc: 0.19759999215602875 | acc_grad: [0.00534246]                        \n",
      "=================================\n",
      "Epoch: 3\n",
      "=================================\n",
      "loss: 1.7694549560546875 | acc: 0.3346000015735626 | acc_grad: [0.00791738]                         \n",
      "=================================\n",
      "Epoch: 4\n",
      "=================================\n",
      "loss: 1.6861920356750488 | acc: 0.42179998755455017 | acc_grad: [0.00558862]                        \n",
      "=================================\n",
      "Epoch: 5\n",
      "=================================\n",
      "loss: 1.6046465635299683 | acc: 0.483599990606308 | acc_grad: [0.00523523]                          \n",
      "=================================\n",
      "Epoch: 6\n",
      "=================================\n",
      "loss: 1.1855528354644775 | acc: 0.5515999794006348 | acc_grad: [0.00390815]                         \n",
      "=================================\n",
      "Epoch: 7\n",
      "=================================\n",
      "loss: 1.2513983249664307 | acc: 0.5920000076293945 | acc_grad: [0.00372723]                        \n",
      "=================================\n",
      "Epoch: 8\n",
      "=================================\n",
      "loss: 0.8749487400054932 | acc: 0.6211999654769897 | acc_grad: [0.00264062]                        \n",
      "=================================\n",
      "Epoch: 9\n",
      "=================================\n",
      "loss: 0.8163394331932068 | acc: 0.6588000059127808 | acc_grad: [0.00228831]                        \n",
      "=================================\n",
      "Epoch: 10\n",
      "=================================\n",
      "loss: 0.8594180941581726 | acc: 0.6639999747276306 | acc_grad: [0.00191138]                        \n",
      "=================================\n",
      "Epoch: 11\n",
      "=================================\n",
      "loss: 0.652024507522583 | acc: 0.6959999799728394 | acc_grad: [0.00146969]                         \n",
      "=================================\n",
      "Epoch: 12\n",
      "=================================\n",
      "loss: 0.7544001340866089 | acc: 0.7325999736785889 | acc_grad: [0.00136861]                        \n",
      "=================================\n",
      "Epoch: 13\n",
      "=================================\n",
      "loss: 0.5659312605857849 | acc: 0.7414000034332275 | acc_grad: [0.00059569]                        \n",
      "=================================\n",
      "Epoch: 14\n",
      "=================================\n",
      "loss: 0.7700072526931763 | acc: 0.7379999756813049 | acc_grad: [0.00065846]                        \n",
      "=================================\n",
      "Epoch: 15\n",
      "=================================\n",
      "loss: 0.6617884635925293 | acc: 0.7631999850273132 | acc_grad: [0.00071354]                        \n"
     ]
    }
   ],
   "source": [
    "relu_xavier_normal_loss_arr, relu_xavier_normal_acc_validation, relu_xavier_normal_model, relu_xavier_normal_optimiser = run_test(lambda x: XavierInitialisationNormal(x), activation=torch.nn.ReLU)\n",
    "relu_xavier_normal_model.to(\"cpu\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0Hkpg4-bVUwE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_arr_local(\"relu_xavier_normal_const_var_loss_arr\", relu_xavier_normal_loss_arr)\n",
    "save_arr_local(\"relu_xavier_normal_acc_validation\", relu_xavier_normal_acc_validation)\n",
    "\n",
    "save_net_local(\"relu_xavier_normal_model\", relu_xavier_normal_model)\n",
    "save_net_local(\"relu_xavier_normal_optimiser\", relu_xavier_normal_optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exy0nADRVU81",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
